{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Import Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets that you got from the notebook **Preprocessing for Agreement Reached**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_encoded.csv\", low_memory=False)\n",
    "df_val = pd.read_csv(\"validation_encoded.csv\")\n",
    "df_test = pd.read_csv(\"test_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.set_index('Claim Identifier', inplace=True)\n",
    "df_val.set_index('Claim Identifier', inplace=True)\n",
    "df_test.set_index('Claim Identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to check the index\n",
    "# df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Agreement Reached', axis= 1)\n",
    "y_train = df_train['Agreement Reached']\n",
    "\n",
    "X_val = df_val.drop('Agreement Reached', axis= 1)\n",
    "y_val = df_val['Agreement Reached']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encode Target Variable\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as here to be able to interpret the results properly when a model is tested.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_val_unencoded = y_train.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after undersampling:\n",
      "encoded_target\n",
      "0    54147\n",
      "1    18049\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming X_train and y_train are your original training data\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "class_counts = y_train.value_counts()\n",
    "minority_class = class_counts.idxmin()\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Separate minority and majority data\n",
    "minority_data = training_data[training_data['encoded_target'] == minority_class]\n",
    "majority_data = training_data[training_data['encoded_target'] == majority_class]\n",
    "\n",
    "# Calculate the target size for undersampling (using integer division)\n",
    "target_size = len(minority_data) * 3\n",
    "\n",
    "# Function to undersample a specific class\n",
    "def undersample_class(data, n):\n",
    "    return resample(data, n_samples=n, replace=False, random_state=42)\n",
    "\n",
    "# Undersample the majority class\n",
    "undersampled_majority = undersample_class(majority_data, target_size)\n",
    "\n",
    "# Combine undersampled majority class with minority class\n",
    "balanced_data = pd.concat([undersampled_majority, minority_data])\n",
    "\n",
    "# Separate features and target\n",
    "X_train_balanced = balanced_data.drop(columns='encoded_target')\n",
    "y_train_balanced = balanced_data['encoded_target']\n",
    "\n",
    "# Check class distribution after undersampling\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(y_train_balanced.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've decided to try this model, because it was one of the best models, based on f1_macro, to predict Claim Injury Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:09:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[0]:     \t0.98\n",
      "[1]:     \t0.2\n",
      "\n",
      "Macro f1: 0.589\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.960822405291702\n",
      "Test Score: 0.95666287280498\n",
      "Difference: 0.004159532486722073\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.95666287280498\n",
      "\n",
      "--------- Precision ---------\n",
      "[0]:     \t0.96\n",
      "[1]:     \t0.72\n",
      "\n",
      "Macro precision: 0.84\n",
      "\n",
      "---------- Recall ----------\n",
      "[0]:     \t1.0\n",
      "[1]:     \t0.12\n",
      "\n",
      "Macro recall: 0.557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is not capable of predictiong that well the class 1, we are going to perform the same model with undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:12:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[0]:     \t0.94\n",
      "[1]:     \t0.4\n",
      "\n",
      "Macro f1: 0.668\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.960822405291702\n",
      "Test Score: 0.95666287280498\n",
      "Difference: 0.004159532486722073\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.8880191396450804\n",
      "\n",
      "--------- Precision ---------\n",
      "[0]:     \t0.99\n",
      "[1]:     \t0.27\n",
      "\n",
      "Macro precision: 0.627\n",
      "\n",
      "---------- Recall ----------\n",
      "[0]:     \t0.89\n",
      "[1]:     \t0.79\n",
      "\n",
      "Macro recall: 0.842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model_under = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model_under.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train_under = xgb_model_under.score(X_train_balanced, y_train_balanced)  # Accuracy on training data\n",
    "score_test_under = xgb_model_under.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred_under = xgb_model_under.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred_under, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:18:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[0]:     \t0.94\n",
      "[1]:     \t0.4\n",
      "\n",
      "Macro f1: 0.668\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.8970857111197297\n",
      "Test Score: 0.8880017188516213\n",
      "Difference: 0.00908399226810841\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.8880017188516213\n",
      "\n",
      "--------- Precision ---------\n",
      "[0]:     \t0.99\n",
      "[1]:     \t0.27\n",
      "\n",
      "Macro precision: 0.627\n",
      "\n",
      "---------- Recall ----------\n",
      "[0]:     \t0.89\n",
      "[1]:     \t0.79\n",
      "\n",
      "Macro recall: 0.843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_models = [\n",
    "    ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ") )\n",
    "]\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42) \n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=nn)\n",
    "stacked_model.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train_balanced, y_train_balanced)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python2\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python2\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:20:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:21:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Python2\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[0]:     \t0.98\n",
      "[1]:     \t0.19\n",
      "\n",
      "Macro f1: 0.583\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.9604052213336818\n",
      "Test Score: 0.9567035213230511\n",
      "Difference: 0.0037017000106306375\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.9567035213230511\n",
      "\n",
      "--------- Precision ---------\n",
      "[0]:     \t0.96\n",
      "[1]:     \t0.75\n",
      "\n",
      "Macro precision: 0.855\n",
      "\n",
      "---------- Recall ----------\n",
      "[0]:     \t1.0\n",
      "[1]:     \t0.11\n",
      "\n",
      "Macro recall: 0.553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_models = [\n",
    "    ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ") )\n",
    "]\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42) \n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=nn)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our predictions, For the model without Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on X_test\n",
    "xgb_test_predictions = xgb_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability scores for each class\n",
    "xgb_test_probabilities = xgb_model.predict_proba(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions \n",
    "agreement_reached_predictions = pd.DataFrame({\n",
    "    'Agreement Reached': xgb_test_predictions\n",
    "}, index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our predictions, For the model with Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on X_test\n",
    "xgb_test_predictions_under = xgb_model_under.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability scores for each class\n",
    "xgb_test_probabilities_under = xgb_model_under.predict_proba(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions \n",
    "agreement_reached_predictions_under = pd.DataFrame({\n",
    "    'Agreement Reached': xgb_test_predictions_under\n",
    "}, index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s with undersampling: 357911\n",
      "Number of 1s with undersampling: 30064\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of 0s with undersampling: {len(agreement_reached_predictions_under[agreement_reached_predictions_under['Agreement Reached'] == 0])}\")\n",
    "print(f\"Number of 1s with undersampling: {len(agreement_reached_predictions_under[agreement_reached_predictions_under['Agreement Reached'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s without undersampling: 386982\n",
      "Number of 1s without undersampling: 993\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of 0s without undersampling: {len(agreement_reached_predictions[agreement_reached_predictions['Agreement Reached'] == 0])}\")\n",
    "print(f\"Number of 1s without undersampling: {len(agreement_reached_predictions[agreement_reached_predictions['Agreement Reached'] == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement_reached_predictions.to_csv('Agreement_Reached_Predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions for XGBoost with Undersampling\n",
    "Since our main goal is to maximize the f1_macro the one chosen was this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_reached_predictions_under.to_csv('Agreement_Reached_Predictions_under.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.concat([df_test, agreement_reached_predictions], axis=1)\n",
    "# df_test.to_csv('df_test_AR.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
