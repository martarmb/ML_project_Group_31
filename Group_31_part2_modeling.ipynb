{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group 31 Modeling Notebook** <br>\n",
    "* Ana Margarida Valente, nr 20240936\n",
    "* Eduardo Mendes, nr 20240850\n",
    "* Julia Karpienia, nr 20240514\n",
    "* Marta Boavida, nr 20240519\n",
    "* Victoria Goon, nr 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents** <br>\n",
    "* [0. Import](#imports)\n",
    "* [1. Import Datasets](#importdatasets)\n",
    "    * [1.1 Encoding Target](#encoding)\n",
    "* [2. Sampling Datasets](#sampling)\n",
    "    * [2.1 Undersampling](#undersampling)\n",
    "    * [2.2 Oversampling](#oversampling)\n",
    "* [3. Model](#model)\n",
    "    * [3.1. Logistic Regression](#lr)\n",
    "    * [3.2. Decision Tree](#dt)\n",
    "    * [3.3. K Nearest Neighbors](#knn)\n",
    "    * [3.4. Gaussian Naive Bayes](#gnb)\n",
    "    * [3.5. Neural Networks](#nn)\n",
    "* [4. Ensemble Models](#ensemble)\n",
    "    * [4.1. Boosted Models]\n",
    "        * [4.1.1. RandomForest](#rf)\n",
    "        * [4.1.2. XGBoost](#xgb)\n",
    "        * [4.1.3. Gradient Boosted Decision Trees](#gbdt)\n",
    "    * [4.2. BaggingClassifier Models]\n",
    "        * [4.2.1. Bagging XGBoost](#baggingxgb)\n",
    "        * [4.2.2. Bagging MLP](#baggingmlp)\n",
    "        * [4.2.3. Bagging Logistic Regression](#bagginglr)\n",
    "    * [4.3. StackingClassifier Models](#stacking)\n",
    "* [5. Kaggle Submission](#kaggle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"imports\">\n",
    "\n",
    "## 0. Import Packages\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Import models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## Import ensemble models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import imbalanced data methods\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Settings\n",
    "sns.set()\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) #Show all columns\n",
    "\n",
    "## Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import imbalaned-learn package for xgboost\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>\n",
    "\n",
    "Importing the datasets that were created and saved from the preprocessing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the training, valaidation, and test datasets that were saved from the preprocessing dataset\n",
    "# train_data = pd.read_csv(\"train_encoded.csv\", low_memory=False)\n",
    "# validation_data = pd.read_csv(\"validation_encoded.csv\", low_memory=False)\n",
    "# test_data = pd.read_csv(\"test_encoded.csv\")\n",
    "\n",
    "# Import the training, valaidation, and test datasets that were saved from the preprocessing dataset\n",
    "train_data = pd.read_csv(\"train_encoded_std_agreach.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"validation_encoded_std_agreach.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"test_encoded_std_agreach.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Claim Identifiers as the index\n",
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "validation_data = validation_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target variable from the features in both train and validation\n",
    "X_train = train_data.drop('Claim Injury Type', axis = 1)\n",
    "y_train = train_data['Claim Injury Type']\n",
    "\n",
    "X_val = validation_data.drop('Claim Injury Type', axis = 1)\n",
    "y_val = validation_data['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"encoding\">\n",
    "\n",
    "### 1.1 Encode Target Variable\n",
    "\n",
    "</a>\n",
    "\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as this modeling notebook to be able to interpret the results properly when a model is tested with the KaggleSubmission csv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# create a copy of the unencoded target to use when assessing the data - makes it more human readable\n",
    "y_val_unencoded = y_val.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sampling\">\n",
    "\n",
    "## 2. Sampling Techniques\n",
    "\n",
    "</a>\n",
    "\n",
    "As the WCB dataset predictions have a total of xxx rows, below is the distribution of the classes:\n",
    "1. CANCELLED: xxx\n",
    "2. NON-COMP: xxx\n",
    "3. MED ONLY: xxx\n",
    "4. TEMPORARY: xxx\n",
    "5. PPD SCH LOSS: xxx\n",
    "6. PPD NSL: xxx\n",
    "7. PTD: xxx\n",
    "8. DEATH: xxx\n",
    "\n",
    "<br/>Class 7 is significantly lower than the others which will cause the models to not fully train. The following cells is to help address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"undersampling\">\n",
    "\n",
    "#### Undersampling <br/>\n",
    "\n",
    "</a>\n",
    "The below code takes the size of the minority class + half of the minority class and randomly creates subsamples of each of the majority classes along with all of the minoirty class and creates a new dataset from the amalgamation of those datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the encoded variables back to the x set\n",
    "# training_data_undersampled = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_classes = {}\n",
    "# for x in range(0,8):\n",
    "#     if x != 6:\n",
    "#         majority_classes[x] = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == x]\n",
    "\n",
    "# minority_class = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == 6]\n",
    "\n",
    "# size = int(len(minority_class) + (len(minority_class) * 0.5))\n",
    "\n",
    "# print(size)\n",
    "\n",
    "# # Perform undersampling\n",
    "# undersampled_majority_0 = majority_classes[0].sample(n=size, random_state=42)\n",
    "# undersampled_majority_1 = majority_classes[1].sample(n=size, random_state=42)\n",
    "# undersampled_majority_2 = majority_classes[2].sample(n=size, random_state=42)\n",
    "# undersampled_majority_3 = majority_classes[3].sample(n=size, random_state=42)\n",
    "# undersampled_majority_4 = majority_classes[4].sample(n=size, random_state=42)\n",
    "# undersampled_majority_5 = majority_classes[5].sample(n=size, random_state=42)\n",
    "# undersampled_majority_7 = majority_classes[7].sample(n=size, random_state=42)\n",
    "# # undersampled_majority.head()\n",
    "# balanced_data = pd.concat([undersampled_majority_0, undersampled_majority_1, undersampled_majority_2, \n",
    "#                            undersampled_majority_3, undersampled_majority_4, undersampled_majority_5, \n",
    "#                            minority_class, undersampled_majority_7])\n",
    "\n",
    "# # Separate features and target\n",
    "# X_train = balanced_data.drop(columns='encoded_target')\n",
    "# y_train = balanced_data['encoded_target']\n",
    "\n",
    "# # Check class distribution after undersampling\n",
    "# print(\"Class distribution after undersampling:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"oversampling\">\n",
    "\n",
    "#### Oversampling\n",
    "\n",
    "</a>\n",
    "The below cell takes the minority class(es) and created synthetic data for it to match the remaining majority class(es). It uses SMOTE (Synthetic Minority Oversampling TEchnique).\n",
    "<br/>\n",
    "<a href=\"https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\">https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling each at: 203607\n"
     ]
    }
   ],
   "source": [
    "# Initialize SMOTE\n",
    "# sampling_strategy=auto : 'not majority' - specifies the class targeted by the resampling\n",
    "# k_neighbors default set to 5. Tried 3 and 10, but 5 had best f1 scores.\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42) #k_neighbors=3\n",
    "\n",
    "# Fit and resample the dataset\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class distribution after oversampling each at:\", y_train.value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "## 3. Model\n",
    "</a>\n",
    "\n",
    "Type of Problem: Multiclassification Problem\n",
    "<br/>\n",
    "Metric used:<br/>\n",
    "As a classification problem, we observed the following metrics to determine the effectiveness of our model:\n",
    " - accuracy\n",
    " - precision\n",
    " - recall\n",
    " - macro f1 score\n",
    "\n",
    " Each point is measured in a different and observing them all allows us to get an accurate view of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lr\">\n",
    "\n",
    "#### Logistic Regression\n",
    "</a>\n",
    "\n",
    "<br/>Grid Search - Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs'], 'class_weight': [None, 'balanced']}\n",
    "# grid_search = GridSearchCV(LogisticRegression(multi_class='multinomial', random_state=42), param_grid, cv=5, scoring='f1_macro')\n",
    "# grid_search.fit(X_train_std_scaler_encoded, Y_train_encoded_df)\n",
    "\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = lr_model.score(X_train, y_train)\n",
    "score_test = lr_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "lr_y_pred = lr_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, lr_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dt\">\n",
    "\n",
    "#### DECISION TREE\n",
    "\n",
    "</a>\n",
    "\n",
    "<br/>Gridsearch - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Decision Tree Classifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],                          \n",
    "#     'splitter': ['best', 'random'],                             \n",
    "#     'max_depth': [None, 10, 20, 30],                           \n",
    "#     'min_samples_split': [2, 5, 10],                            \n",
    "#     'min_samples_leaf': [1, 2, 4],                              \n",
    "#     'max_features': [None, 'sqrt', 'log2'],                    \n",
    "#     'max_leaf_nodes': [None, 10, 20, 30],                       \n",
    "#     'min_impurity_decrease': [0.0, 0.1, 0.2]                   \n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV:\n",
    "# grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# #Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# #Best Score: 0.7769977245887005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test History\n",
    "# 0.366\n",
    "# (oversampling) - 0.343 - 21s\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  \n",
    "    max_depth=10, \n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease= 0.0,\n",
    "    min_samples_leaf= 1,\n",
    "    min_samples_split=2,\n",
    "    splitter='best',  \n",
    "    random_state=42    \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = decision_tree.score(X_train, y_train)\n",
    "score_test = decision_tree.score(X_val, y_val)\n",
    "\n",
    "# Make predictions\n",
    "dt_y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, dt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"knn\">\n",
    "\n",
    "#### K Nearest Neighbors\n",
    "</a>\n",
    "<br/>Grid Search - KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for Randomized Search\n",
    "# param_distributions = {\n",
    "#     'n_neighbors' : [5,10], \n",
    "#     'leaf_size': [30, 50],                                  \n",
    "#     'metric': ['euclidean', 'manhattan'],          \n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV with KNN classifier\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=KNeighborsClassifier(),\n",
    "#     param_distributions=param_distributions,\n",
    "#     n_iter=5,                                     \n",
    "#     cv=2,                                          \n",
    "#     scoring='f1_macro',                            \n",
    "#     verbose=2,                                     \n",
    "#     random_state=42,                               \n",
    "#     n_jobs=-1                                      \n",
    "# )\n",
    "\n",
    "# # Fit the random search to the training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# # Best Parameters: {'n_neighbors': 5, 'metric': 'euclidean', 'leaf_size': 30}\n",
    "# # Best Score: 0.328445945439427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - KNN<br/>\n",
    "- KNN will be commented out\n",
    "- KNN is not appropriate for too large datasets. Too computational expensive due to memorization requirements.\n",
    "- KNN takes too long to process due to our large dataset. (Several optimization attempts were conducted to optimize the algorithm namely with KD-Tree and Ball-Tree, however the\n",
    "algorithm remains too computational expensive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test History\n",
    "# # 0.334\n",
    "# # Initialize the KNN Classifier\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5, leaf_size=30, metric='euclidean')  \n",
    "\n",
    "# # Train the model\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Determine the scores for the model for both train and validation sets\n",
    "# score_train = knn_model.score(X_train, y_train)\n",
    "# score_test = knn_model.score(X_val, y_val)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = knn_model.predict(X_val)\n",
    "\n",
    "# # Display the model metrics using the score_model function\n",
    "# score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"gnb\">\n",
    "\n",
    "#### Model - Gaussian Naive Bayes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = GaussianNB()\n",
    "\n",
    "# fit the model to the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# determine the scores for the model for both train and validation\n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_val, y_val)\n",
    "\n",
    "# use model to predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# display the model metrics\n",
    "score_model(y_val, y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"nn\">\n",
    "\n",
    "#### Neural Network (MLPClassifier):\n",
    "</a>\n",
    "<br/>GridSearch - MLPClasssifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {  \n",
    "#     'hidden_layer_sizes': [\n",
    "#         (50),        # Larger single-layer model\n",
    "#         (50, 30),      # Moderate two-layer model\n",
    "#         (100, 50),     # Larger two-layer model\n",
    "#         (128, 64, 32)  # Complex three-layer model\n",
    "#     ],\n",
    "#     'activation': ['relu', 'logistic'],                \n",
    "#     'solver': ['adam', 'sgd'],                     \n",
    "#     'alpha': [0.0001, 0.001],                       \n",
    "#     'learning_rate': ['adaptive', 'invscaling']          \n",
    "# }\n",
    "\n",
    "# # Initialize the Neural Network model\n",
    "# mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# # Initialize Random Search for hyperparameter tuning\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=mlp,\n",
    "#     param_distributions=param_grid,  # Using param_distributions for randomized search\n",
    "#     n_iter=10,  # Number of random combinations to try\n",
    "#     cv=2,  # 3-fold cross-validation\n",
    "#     scoring='f1_macro',  # Evaluation metric\n",
    "#     verbose=2,  # Display progress logs\n",
    "#     n_jobs=-1,  # Use all available processors for parallel computation\n",
    "#     random_state=42  # For reproducibility\n",
    "# )\n",
    "\n",
    "# # Fit the randomized search to the training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# \n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# # Best Parameters: {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 30), 'alpha': 0.001, 'activation': 'logistic'}\n",
    "# # Best Score: 0.4192846184298069\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL - MLPClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with agreement reached - 0.389 - 14m 57s\n",
    "\n",
    "# # Initialize the Neural Network model\n",
    "model = MLPClassifier(hidden_layer_sizes=(50, 30),  \n",
    "                      activation='logistic',        \n",
    "                      solver='adam',               \n",
    "                      alpha=0.0001,                \n",
    "                      learning_rate='adaptive',    \n",
    "                      max_iter=200,                \n",
    "                      random_state=42)             \n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ensemble\">\n",
    "\n",
    "### Ensemble Models\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"rf\">\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "</a>\n",
    "Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.379\n",
    "# (oversampling) - 0.433 (overfitting w/ 0.23 diff) - 6m 5s\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = rf_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = rf_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "rf_y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, rf_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"xgb\">\n",
    "\n",
    "#### XGBoost\n",
    "</a>\n",
    "Also using decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.59\n",
      "[2. NON-COMP]:     \t0.91\n",
      "[3. MED ONLY]:     \t0.18\n",
      "[4. TEMPORARY]:     \t0.79\n",
      "[5. PPD SCH LOSS]:     \t0.66\n",
      "[6. PPD NSL]:     \t0.13\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.48\n",
      "\n",
      "Macro f1: 0.469\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.8804707107319493\n",
      "Test Score: 0.7864559137786863\n",
      "Difference: 0.09401479695326298\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7864559137786863\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.62\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.45\n",
      "[4. TEMPORARY]:     \t0.75\n",
      "[5. PPD SCH LOSS]:     \t0.62\n",
      "[6. PPD NSL]:     \t0.14\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.38\n",
      "\n",
      "Macro precision: 0.479\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.57\n",
      "[2. NON-COMP]:     \t0.95\n",
      "[3. MED ONLY]:     \t0.12\n",
      "[4. TEMPORARY]:     \t0.84\n",
      "[5. PPD SCH LOSS]:     \t0.72\n",
      "[6. PPD NSL]:     \t0.13\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.65\n",
      "\n",
      "Macro recall: 0.496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.442\n",
    "# (oversampling) 0.453 (overfit by 0.10 diff) 4m 18s\n",
    "# (oversampling) 0.469 (overfit by 0.09 diff) 3m 28s - with agreement reached\n",
    "\n",
    "# max_depth = 19, n_estimators = 150, lr = 0.6 -> overfitting\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"gbdt\">\n",
    "\n",
    "#### Gradient Boosted Decision Trees\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 min = max_depth = 6 - .402 f1\n",
    "\n",
    "gbdt_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Number of boosting stages\n",
    "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
    "    max_depth=6,            # Limits depth of each tree to prevent overfitting\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = gbdt_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = gbdt_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "gbdt_y_pred = gbdt_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, gbdt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"baggingxgb\">\n",
    "\n",
    "#### Bagging XGBoost\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with correct hyperparameters - 0.451 - 0.104 for overfitting\n",
    "# oversampling - 0.448 - 17m 59s - 0.107 diff for overfitting\n",
    "# 0.425 f1 macro score\n",
    "bagging_model_xgb = BaggingClassifier(estimator=xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    "), n_estimators=10, random_state=42)\n",
    "bagging_model_xgb.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model_xgb.predict(X_val)\n",
    "\n",
    "score_train = bagging_model_xgb.score(X_train, y_train)\n",
    "score_test = bagging_model_xgb.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"baggingmlp\">\n",
    "\n",
    "#### Bagging MLPClassifier\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hidden_layer_sizes=(13,), max_iter=500, random_state=42) - 0.395 (no overfit) 12m 58s\n",
    "# (hidden_layer_sizes=(15,), max_iter=500, random_state=42) - 0.407 (no overfit) 11m 34s\n",
    "# (hidden_layer_sizes=(20,), max_iter=500, random_state=42) - 0.407 (no overfit) 15m 20s\n",
    "# (hidden_layer_sizes=(10,), max_iter=500, random_state=42) - 0.389 (no overfit) 4m 6s\n",
    "# (hidden_layer_sizes=(10,), max_iter=1000, random_state=42) - 0.389 (no overfit) 4m 8s\n",
    "base_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility)\n",
    "bagging_model_mlpc = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)\n",
    "bagging_model_mlpc.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model_mlpc.predict(X_val)\n",
    "\n",
    "score_train = bagging_model_mlpc.score(X_train, y_train)\n",
    "score_test = bagging_model_mlpc.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bagginglr\">\n",
    "\n",
    "#### Bagging Logistic Regression\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.38\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.27\n",
      "[4. TEMPORARY]:     \t0.57\n",
      "[5. PPD SCH LOSS]:     \t0.62\n",
      "[6. PPD NSL]:     \t0.13\n",
      "[7. PTD]:     \t\t0.01\n",
      "[8. DEATH]:     \t0.14\n",
      "\n",
      "Macro f1: 0.372\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.747581738348878\n",
      "Test Score: 0.6408935705658274\n",
      "Difference: 0.10668816778305057\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.6408935705658274\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.25\n",
      "[2. NON-COMP]:     \t0.89\n",
      "[3. MED ONLY]:     \t0.25\n",
      "[4. TEMPORARY]:     \t0.82\n",
      "[5. PPD SCH LOSS]:     \t0.54\n",
      "[6. PPD NSL]:     \t0.07\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.08\n",
      "\n",
      "Macro precision: 0.364\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.74\n",
      "[2. NON-COMP]:     \t0.8\n",
      "[3. MED ONLY]:     \t0.3\n",
      "[4. TEMPORARY]:     \t0.44\n",
      "[5. PPD SCH LOSS]:     \t0.73\n",
      "[6. PPD NSL]:     \t0.68\n",
      "[7. PTD]:     \t\t0.14\n",
      "[8. DEATH]:     \t0.8\n",
      "\n",
      "Macro recall: 0.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13m - 0.372 - overfit by 0.1 diff\n",
    "base_model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "bagging_model_lr = BaggingClassifier(estimator=base_model_lr, n_estimators=10, random_state=42)\n",
    "bagging_model_lr.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model_lr.predict(X_val)\n",
    "\n",
    "score_train = bagging_model_lr.score(X_train, y_train)\n",
    "score_test = bagging_model_lr.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stacking\">\n",
    "\n",
    "#### Stacking\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.59\n",
      "[2. NON-COMP]:     \t0.9\n",
      "[3. MED ONLY]:     \t0.14\n",
      "[4. TEMPORARY]:     \t0.8\n",
      "[5. PPD SCH LOSS]:     \t0.65\n",
      "[6. PPD NSL]:     \t0.02\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.5\n",
      "\n",
      "Macro f1: 0.45\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.8835250015962123\n",
      "Test Score: 0.7877218247700455\n",
      "Difference: 0.09580317682616679\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7877218247700455\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.61\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.42\n",
      "[4. TEMPORARY]:     \t0.74\n",
      "[5. PPD SCH LOSS]:     \t0.65\n",
      "[6. PPD NSL]:     \t0.11\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.48\n",
      "\n",
      "Macro precision: 0.483\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.57\n",
      "[2. NON-COMP]:     \t0.96\n",
      "[3. MED ONLY]:     \t0.08\n",
      "[4. TEMPORARY]:     \t0.87\n",
      "[5. PPD SCH LOSS]:     \t0.64\n",
      "[6. PPD NSL]:     \t0.01\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.54\n",
      "\n",
      "Macro recall: 0.459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.440 - LR -> XGB -> MLP w/ a 0.015 difference in scores (3m 55s)\n",
    "# 0.425 - LR -> MLP -> XGB w/ a 0.0099 difference in scores (37m)\n",
    "# 0.410 - MLP -> XGB -> GBC w/ a 0.011 difference in scores (93m 35s)\n",
    "\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ") )\n",
    "]\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42) \n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=nn)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lr_y_pred_f1   = f1_score(y_val, lr_y_pred, average='macro')\n",
    "# # dt_y_pred_f1   = f1_score(y_val, dt_y_pred, average='macro')\n",
    "# # knn_y_pred_f1  = f1_score(y_val, knn_y_pred, average='macro')\n",
    "# mplc_y_pred_f1 = f1_score(y_val, mplc_y_pred, average='macro')\n",
    "# # rf_y_pred_f1   = f1_score(y_val, rf_y_pred, average='macro')\n",
    "# xgb_y_pred_f1  = f1_score(y_val, xgb_y_pred, average='macro')\n",
    "# gbdt_y_pred_f1 = f1_score(y_val, gbdt_y_pred, average='macro')\n",
    "\n",
    "# # f1_score(y_actual, y_predicted, average='macro')\n",
    "\n",
    "# # Assign weights based on F1 scores\n",
    "# #weights = [lr_y_pred_f1, dt_y_pred_f1, knn_y_pred_f1, mplc_y_pred_f1, rf_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = [mplc_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "\n",
    "# # Make weighted predictions\n",
    "# # lr_probs    = lr_model.predict_proba(X_val)[:, 1]\n",
    "# # dt_probs    = decision_tree.predict_proba(X_val)[:, 1]\n",
    "# # knn_probs   = knn_model.predict_proba(X_val)[:, 1]\n",
    "# mplc_probs  = mlpc_model.predict_proba(X_val)[:, 1]\n",
    "# # rf_probs    = rf_model.predict_proba(X_val)[:, 1]\n",
    "# xgb_probs   = xgb_model.predict_proba(X_val)[:, 1]\n",
    "# gbdt_probs  = gbdt_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# # Aggregate predictions using weights\n",
    "# weighted_probs = (\n",
    "#                     # weights[0] * lr_probs +\n",
    "#                 #   weights[1] * dt_probs +\n",
    "#                 #   weights[2] * knn_probs +\n",
    "#                   weights[0] * mplc_probs + \n",
    "#                 #   weights[4] * rf_probs + \n",
    "#                   weights[1] * xgb_probs + \n",
    "#                   weights[2] * gbdt_probs)\n",
    "\n",
    "# # Final predictions (threshold = 0.5)\n",
    "# final_predictions = (weighted_probs >= 0.2).astype(int)\n",
    "\n",
    "# # Evaluate the ensemble\n",
    "# final_f1 = f1_score(y_val, final_predictions, average='macro')\n",
    "# print(f\"Weighted Ensemble F1 Score: {final_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kaggle\">\n",
    "\n",
    "## 5. Kaggle Submission\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "y_pred_test = bagging_model_lr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2. NON-COMP', '2. NON-COMP', '2. NON-COMP', ..., '1. CANCELLED',\n",
       "       '1. CANCELLED', '1. CANCELLED'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # decode the prediction labels back to their original values\n",
    "decoded_labels = label_encoder.inverse_transform(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6165911</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6166141</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6165907</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6166047</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6166102</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Claim Identifier Claim Injury Type\n",
       "0           6165911       2. NON-COMP\n",
       "1           6166141       2. NON-COMP\n",
       "2           6165907       2. NON-COMP\n",
       "3           6166047       2. NON-COMP\n",
       "4           6166102       2. NON-COMP"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # combine the prediction values with their claim identifiers into a dataframe\n",
    "kaggle_submission = pd.DataFrame({\"Claim Identifier\": test_data.index, \"Claim Injury Type\":decoded_labels})\n",
    "kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the resulting dataframe into a csv file named \"Kaggle_submission.csv\"\n",
    "# this will be found in the directory the file is currently running from\n",
    "# if a file exists with the same name, it will overwrite it with the new output.\n",
    "kaggle_submission.to_csv(\"Kaggle_Submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
