{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group 31** <br>\n",
    "* Ana Margarida Valente, nr 20240936\n",
    "* Eduardo Mendes, nr 20240850\n",
    "* Julia Karpienia, nr 20240514\n",
    "* Marta Boavida, nr 20240519\n",
    "* Victoria Goon, nr 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Import datetime object for date columns in dataset\n",
    "from datetime import datetime\n",
    "\n",
    "## Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "from math import ceil\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Import Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) #Show all columns\n",
    "\n",
    "## Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_encoded.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"validation_encoded.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"test_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "validation_data = validation_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Claim Injury Type', axis = 1)\n",
    "y_train = train_data['Claim Injury Type']\n",
    "\n",
    "X_val = validation_data.drop('Claim Injury Type', axis = 1)\n",
    "y_val = validation_data['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1. CANCELLED]:     \t0.69\n",
    "# [2. NON-COMP]:     \t0.84\n",
    "# [3. MED ONLY]:     \t0.49\n",
    "# [4. TEMPORARY]:     \t0.69\n",
    "# [5. PPD SCH LOSS]:     \t0.61\n",
    "# [6. PPD NSL]:     \t0.14\n",
    "# [7. PTD]:     \t\t0.0\n",
    "# [8. DEATH]:     \t0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encode Target Variable\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as here to be able to interpret the results properly when a model is tested.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_val_unencoded = y_train.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the encoded variables back to the x set\n",
    "# training_data_undersampled = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_classes = {}\n",
    "# for x in range(0,8):\n",
    "#     if x != 6:\n",
    "#         majority_classes[x] = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == x]\n",
    "\n",
    "# minority_class = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == 6]\n",
    "\n",
    "# size = int(len(minority_class) + (len(minority_class) * 2))\n",
    "\n",
    "# print(size)\n",
    "\n",
    "# # Perform undersampling\n",
    "# undersampled_majority_0 = majority_classes[0].sample(n=size, random_state=42)\n",
    "# undersampled_majority_1 = majority_classes[1].sample(n=size, random_state=42)\n",
    "# undersampled_majority_2 = majority_classes[2].sample(n=size, random_state=42)\n",
    "# undersampled_majority_3 = majority_classes[3].sample(n=size, random_state=42)\n",
    "# undersampled_majority_4 = majority_classes[4].sample(n=size, random_state=42)\n",
    "# undersampled_majority_5 = majority_classes[5].sample(n=size, random_state=42)\n",
    "# undersampled_majority_7 = majority_classes[7].sample(n=size, random_state=42)\n",
    "# # undersampled_majority.head()\n",
    "# balanced_data = pd.concat([undersampled_majority_0, undersampled_majority_1, undersampled_majority_2, \n",
    "#                            undersampled_majority_3, undersampled_majority_4, undersampled_majority_5, \n",
    "#                            minority_class, undersampled_majority_7])\n",
    "\n",
    "# # Separate features and target\n",
    "# X_train_balanced = balanced_data.drop(columns='encoded_target')\n",
    "# y_train_balanced = balanced_data['encoded_target']\n",
    "\n",
    "# # Check class distribution after undersampling\n",
    "# print(\"Class distribution after undersampling:\", y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "## 2. Model\n",
    "</a>\n",
    "\n",
    "Type of Problem <br/>\n",
    "The type of problem to be solved is a multiclassification problem where the output is between 8 different choices. We will use a simple Logistical Regression model set to be able to compute multiple classes.<br/>\n",
    "<br/>\n",
    "Metric used:<br/>\n",
    "As a classification problem, we observed the following metrics to determine the effectiveness of our model:\n",
    " - accuracy\n",
    " - precision\n",
    " - recall\n",
    " - f1 score\n",
    "\n",
    " Each point is measured in a different and observing them all allows us to get an accurate view of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop(['Season_of_Accident', 'Age_Group', 'Industry_Avg_Weekly_Wage','COVID_Age'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.drop([\"Age_at_Assembly\", 'Season_of_Accident', 'Age_Group','COVID_Age'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models from class\n",
    "- Logistic Regression\n",
    "- Decision Tree (J48)\n",
    "- K-Nearest Neighbor\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We used GridSearchCV to find the optimal parameter values for our final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs'], 'class_weight': [None, 'balanced']}\n",
    "# grid_search = GridSearchCV(LogisticRegression(multi_class='multinomial', random_state=42), param_grid, cv=5, scoring='f1_macro')\n",
    "# grid_search.fit(X_train_std_scaler_encoded, Y_train_encoded_df)\n",
    "\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of running GridSearch for LogisticRegression with diffenret parameters we got the parameters that get the best macro F1 score for our model. Those parameters are:\n",
    "Best Parameters: {'C': 10, 'class_weight': None, 'solver': 'lbfgs'}\n",
    "WE applied those parameters to our LogisticRegression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression after GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.52\n",
      "[2. NON-COMP]:     \t0.89\n",
      "[3. MED ONLY]:     \t0.11\n",
      "[4. TEMPORARY]:     \t0.72\n",
      "[5. PPD SCH LOSS]:     \t0.53\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.25\n",
      "\n",
      "Macro f1: 0.379\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.7482700048746413\n",
      "Test Score: 0.7463067917866766\n",
      "Difference: 0.001963213087964766\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7463067917866766\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.66\n",
      "[2. NON-COMP]:     \t0.82\n",
      "[3. MED ONLY]:     \t0.29\n",
      "[4. TEMPORARY]:     \t0.67\n",
      "[5. PPD SCH LOSS]:     \t0.6\n",
      "[6. PPD NSL]:     \t0.06\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.56\n",
      "\n",
      "Macro precision: 0.459\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.43\n",
      "[2. NON-COMP]:     \t0.96\n",
      "[3. MED ONLY]:     \t0.07\n",
      "[4. TEMPORARY]:     \t0.78\n",
      "[5. PPD SCH LOSS]:     \t0.48\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.16\n",
      "\n",
      "Macro recall: 0.361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = lr_model.score(X_train, y_train)\n",
    "score_test = lr_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "lr_y_pred = lr_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, lr_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "confusion_matrix(y_val, lr_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION TREE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Create a DecisionTreeClassifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],                          # Split criterion\n",
    "#     'splitter': ['best', 'random'],                             # Splitting strategy\n",
    "#     'max_depth': [None, 10, 20, 30],                            # Max depth of the tree\n",
    "#     'min_samples_split': [2, 5, 10],                            # Minimum samples to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4],                              # Minimum samples at a leaf node\n",
    "#     'max_features': [None, 'sqrt', 'log2'],                     # Max features to consider for splits\n",
    "#     'max_leaf_nodes': [None, 10, 20, 30],                       # Max number of leaf nodes\n",
    "#     'min_impurity_decrease': [0.0, 0.1, 0.2]                   # Minimum impurity decrease to split\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV with 5-fold cross-validation and scoring based on accuracy\n",
    "# grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# # You can also access the best model found\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# #Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# #Best Score: 0.7769977245887005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  # 'gini' for Gini Impurity or 'entropy' for Information Gain\n",
    "    max_depth=10, \n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease= 0.0,\n",
    "    min_samples_leaf= 1,\n",
    "    min_samples_split=2,\n",
    "    splitter='best',  # Maximum depth of the tree (None means no limit)\n",
    "    random_state=42    # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = decision_tree.score(X_train, y_train)\n",
    "score_test = decision_tree.score(X_val, y_val)\n",
    "\n",
    "# Make predictions\n",
    "dt_y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, dt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # Define the parameter grid for KNN\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 10, 15],                 \n",
    "#     'algorithm': ['brute', 'kd_tree'],             \n",
    "#     'metric': ['euclidean', 'manhattan', 'minkowski'], \n",
    "#     'weights': ['uniform', 'distance']             \n",
    "# }\n",
    "\n",
    "# # Set up the GridSearchCV with KNN classifier\n",
    "# grid_search = GridSearchCV(\n",
    "#     KNeighborsClassifier(),\n",
    "#     param_grid,\n",
    "#     cv=5,                                         \n",
    "#     scoring='f1_macro'                                                      \n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - KNN<br/>\n",
    "KNN takes too long to process due to our large dataset so will be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the KNN model\n",
    "# n_neighbors specifies the number of neighbors to use for classification\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  \n",
    "\n",
    "# Fit the model to the training set\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = knn_model.score(X_train, y_train)\n",
    "score_test = knn_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "knn_y_pred = knn_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, knn_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(64,), (64, 32), (128, 64)],  # Different hidden layer architectures\n",
    "#     'activation': ['relu', 'tanh'],                     # Activation functions\n",
    "#     'solver': ['adam', 'sgd'],                          # Optimizers\n",
    "#     'alpha': [0.0001, 0.001, 0.01],                     # L2 regularization (alpha)\n",
    "#     'learning_rate_init': [0.001, 0.01, 0.1],           # Learning rates\n",
    "# }\n",
    "\n",
    "# # Create the MLPClassifier model\n",
    "# mlp = MLPClassifier(max_iter=200, random_state=42)  # Keeping max_iter constant at 200\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=mlp,\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=5,  # 5-fold cross-validation\n",
    "#                            scoring='f1_macro',  # Evaluation metric\n",
    "#                            verbose=2,           # Display progress logs\n",
    "#                            n_jobs=-1)           # Use all available processors\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score of 0.415\n",
    "\n",
    "# Create the model\n",
    "mlpc_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility\n",
    "\n",
    "# Fit the model to the training set\n",
    "mlpc_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = mlpc_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = mlpc_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "mplc_y_pred = mlpc_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, mplc_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models found from papers:\n",
    "\n",
    "- Random Forest -> (overfits)\n",
    "- Gradient Boosted Decision Trees\n",
    "- XGBoost\n",
    "- Neural Network Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = rf_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = rf_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "rf_y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, rf_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the XGBoost model\n",
    "# xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage\n",
    "#     'max_depth': [3, 5, 7],  # Maximum tree depth\n",
    "#     'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per tree\n",
    "#     'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
    "#     'gamma': [0, 1, 5],  # Minimum loss reduction to make a split\n",
    "#     'reg_alpha': [0, 0.01, 0.1],  # L1 regularization\n",
    "#     'reg_lambda': [1, 10, 100]  # L2 regularization\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='accuracy',\n",
    "#     cv=3,  # 3-fold cross-validation\n",
    "#     verbose=2,\n",
    "#     n_jobs=-1  # Use all available processors\n",
    "# )\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_val)\n",
    "# print(f\"Test Accuracy: {accuracy_score(y_val, y_pred):.2f}\")\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.sum(y_train == 6) / np.sum(y_train != 6)\n",
    "\n",
    "score = 0\n",
    "score_settings = \"\"\n",
    "\n",
    "for x in range(1,20):\n",
    "    for y in range(50, 151, 10):\n",
    "        for z in np.arange(0, 1.1, 0.1):\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                n_estimators=y,  # Number of trees\n",
    "                learning_rate=z,  # Step size shrinkage\n",
    "                max_depth=x,       # Maximum depth of a tree\n",
    "                random_state=42,   # For reproducibility\n",
    "                use_label_encoder=False,  # Avoid warning for encoding\n",
    "                eval_metric='mlogloss',    # Evaluation metric for multi-class classification\n",
    "                scale_pos_weight = pos_weight\n",
    "            )\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_y_pred = xgb_model.predict(X_val)\n",
    "            f1 = f1_score(y_val, xgb_y_pred, average=\"macro\")\n",
    "\n",
    "            if f1 > score:\n",
    "                score = f1\n",
    "                score_settings = \"max_depth: \" + str(x) + \" | n_estimators: \" + str(y) + \" | lr: \" + str(z)\n",
    "\n",
    "print(score)\n",
    "print(score_settings)\n",
    "\n",
    "# # Train the model scale_pos_weight\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Determine the scores for the model for both train and validation sets\n",
    "# score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "# score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# # Use the model to predict on the validation set\n",
    "# xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# # Display the model metrics using the score_model function\n",
    "# score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    learning_rate=0.1,  # Step size shrinkage\n",
    "    max_depth=3,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Number of boosting stages\n",
    "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
    "    max_depth=3,            # Limits depth of each tree to prevent overfitting\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = gbdt_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = gbdt_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "gbdt_y_pred = gbdt_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, gbdt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_y_pred_f1   = f1_score(y_val, lr_y_pred, average='macro')\n",
    "# dt_y_pred_f1   = f1_score(y_val, dt_y_pred, average='macro')\n",
    "# knn_y_pred_f1  = f1_score(y_val, knn_y_pred, average='macro')\n",
    "mplc_y_pred_f1 = f1_score(y_val, mplc_y_pred, average='macro')\n",
    "# rf_y_pred_f1   = f1_score(y_val, rf_y_pred, average='macro')\n",
    "xgb_y_pred_f1  = f1_score(y_val, xgb_y_pred, average='macro')\n",
    "gbdt_y_pred_f1 = f1_score(y_val, gbdt_y_pred, average='macro')\n",
    "\n",
    "# f1_score(y_actual, y_predicted, average='macro')\n",
    "\n",
    "# Assign weights based on F1 scores\n",
    "#weights = [lr_y_pred_f1, dt_y_pred_f1, knn_y_pred_f1, mplc_y_pred_f1, rf_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "weights = [mplc_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "\n",
    "# Make weighted predictions\n",
    "# lr_probs    = lr_model.predict_proba(X_val)[:, 1]\n",
    "# dt_probs    = decision_tree.predict_proba(X_val)[:, 1]\n",
    "# knn_probs   = knn_model.predict_proba(X_val)[:, 1]\n",
    "mplc_probs  = mlpc_model.predict_proba(X_val)[:, 1]\n",
    "# rf_probs    = rf_model.predict_proba(X_val)[:, 1]\n",
    "xgb_probs   = xgb_model.predict_proba(X_val)[:, 1]\n",
    "gbdt_probs  = gbdt_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Aggregate predictions using weights\n",
    "weighted_probs = (\n",
    "                    # weights[0] * lr_probs +\n",
    "                #   weights[1] * dt_probs +\n",
    "                #   weights[2] * knn_probs +\n",
    "                  weights[0] * mplc_probs + \n",
    "                #   weights[4] * rf_probs + \n",
    "                  weights[1] * xgb_probs + \n",
    "                  weights[2] * gbdt_probs)\n",
    "\n",
    "# Final predictions (threshold = 0.5)\n",
    "final_predictions = (weighted_probs >= 0.2).astype(int)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "final_f1 = f1_score(y_val, final_predictions, average='macro')\n",
    "print(f\"Weighted Ensemble F1 Score: {final_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.425 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=xgb.XGBClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('logestic_regression', LogisticRegression()),\n",
    "    ('mlpc', MLPClassifier() )\n",
    "]\n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=xgb.XGBClassifier())\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kaggle\">\n",
    "\n",
    "## 11. Kaggle Submission\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "# y_pred_test = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode the prediction labels back to their original values\n",
    "# decoded_labels = label_encoder.inverse_transform(y_pred_test)\n",
    "# decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the prediction values with their claim identifiers into a dataframe\n",
    "# kaggle_submission = pd.DataFrame({\"Claim Identifier\": test_data.index, \"Claim Injury Type\":decoded_labels})\n",
    "# kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the resulting dataframe into a csv file named \"Kaggle_submission.csv\"\n",
    "# this will be found in the directory the file is currently running from\n",
    "# if a file exists with the same name, it will overwrite it with the new output.\n",
    "# kaggle_submission.to_csv(\"Kaggle_Submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
