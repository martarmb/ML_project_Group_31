{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group 31** <br>\n",
    "* Ana Margarida Valente, nr 20240936\n",
    "* Eduardo Mendes, nr 20240850\n",
    "* Julia Karpienia, nr 20240514\n",
    "* Marta Boavida, nr 20240519\n",
    "* Victoria Goon, nr 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Import Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) #Show all columns\n",
    "\n",
    "## Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_encoded.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"validation_encoded.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"test_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "validation_data = validation_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Claim Injury Type', axis = 1)\n",
    "y_train = train_data['Claim Injury Type']\n",
    "\n",
    "X_val = validation_data.drop('Claim Injury Type', axis = 1)\n",
    "y_val = validation_data['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encode Target Variable\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as here to be able to interpret the results properly when a model is tested.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_val_unencoded = y_train.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the encoded variables back to the x set\n",
    "# training_data_undersampled = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_classes = {}\n",
    "# for x in range(0,8):\n",
    "#     if x != 6:\n",
    "#         majority_classes[x] = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == x]\n",
    "\n",
    "# minority_class = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == 6]\n",
    "\n",
    "# size = int(len(minority_class) + (len(minority_class) * 2))\n",
    "\n",
    "# print(size)\n",
    "\n",
    "# # Perform undersampling\n",
    "# undersampled_majority_0 = majority_classes[0].sample(n=size, random_state=42)\n",
    "# undersampled_majority_1 = majority_classes[1].sample(n=size, random_state=42)\n",
    "# undersampled_majority_2 = majority_classes[2].sample(n=size, random_state=42)\n",
    "# undersampled_majority_3 = majority_classes[3].sample(n=size, random_state=42)\n",
    "# undersampled_majority_4 = majority_classes[4].sample(n=size, random_state=42)\n",
    "# undersampled_majority_5 = majority_classes[5].sample(n=size, random_state=42)\n",
    "# undersampled_majority_7 = majority_classes[7].sample(n=size, random_state=42)\n",
    "# # undersampled_majority.head()\n",
    "# balanced_data = pd.concat([undersampled_majority_0, undersampled_majority_1, undersampled_majority_2, \n",
    "#                            undersampled_majority_3, undersampled_majority_4, undersampled_majority_5, \n",
    "#                            minority_class, undersampled_majority_7])\n",
    "\n",
    "# # Separate features and target\n",
    "# X_train = balanced_data.drop(columns='encoded_target')\n",
    "# y_train = balanced_data['encoded_target']\n",
    "\n",
    "# # Check class distribution after undersampling\n",
    "# print(\"Class distribution after undersampling:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "## 2. Model\n",
    "</a>\n",
    "\n",
    "Type of Problem <br/>\n",
    "The type of problem to be solved is a multiclassification problem where the output is between 8 different choices. We will use a simple Logistical Regression model set to be able to compute multiple classes.<br/>\n",
    "<br/>\n",
    "Metric used:<br/>\n",
    "As a classification problem, we observed the following metrics to determine the effectiveness of our model:\n",
    " - accuracy\n",
    " - precision\n",
    " - recall\n",
    " - f1 score\n",
    "\n",
    " Each point is measured in a different and observing them all allows us to get an accurate view of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.379\n",
    "\n",
    "# Create the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = lr_model.score(X_train, y_train)\n",
    "score_test = lr_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "lr_y_pred = lr_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, lr_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DecisionTreeClassifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],                          # Split criterion\n",
    "#     'splitter': ['best', 'random'],                             # Splitting strategy\n",
    "#     'max_depth': [None, 10, 20, 30],                            # Max depth of the tree\n",
    "#     'min_samples_split': [2, 5, 10],                            # Minimum samples to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4],                              # Minimum samples at a leaf node\n",
    "#     'max_features': [None, 'sqrt', 'log2'],                     # Max features to consider for splits\n",
    "#     'max_leaf_nodes': [None, 10, 20, 30],                       # Max number of leaf nodes\n",
    "#     'min_impurity_decrease': [0.0, 0.1, 0.2]                   # Minimum impurity decrease to split\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV with 5-fold cross-validation and scoring based on accuracy\n",
    "# grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# # You can also access the best model found\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# #Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# #Best Score: 0.7769977245887005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.366\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  # 'gini' for Gini Impurity or 'entropy' for Information Gain\n",
    "    max_depth=10, \n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease= 0.0,\n",
    "    min_samples_leaf= 1,\n",
    "    min_samples_split=2,\n",
    "    splitter='best',  # Maximum depth of the tree (None means no limit)\n",
    "    random_state=42    # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = decision_tree.score(X_train, y_train)\n",
    "score_test = decision_tree.score(X_val, y_val)\n",
    "\n",
    "# Make predictions\n",
    "dt_y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, dt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for KNN\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 10, 15],                 \n",
    "#     'algorithm': ['brute', 'kd_tree'],             \n",
    "#     'metric': ['euclidean', 'manhattan', 'minkowski'], \n",
    "#     'weights': ['uniform', 'distance']             \n",
    "# }\n",
    "\n",
    "# # Set up the GridSearchCV with KNN classifier\n",
    "# grid_search = GridSearchCV(\n",
    "#     KNeighborsClassifier(),\n",
    "#     param_grid,\n",
    "#     cv=5,                                         \n",
    "#     scoring='f1_macro'                                                      \n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - KNN<br/>\n",
    "KNN takes too long to process due to our large dataset so will be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.334\n",
    "\n",
    "# # Create the KNN model\n",
    "# # n_neighbors specifies the number of neighbors to use for classification\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5)  \n",
    "\n",
    "# # Fit the model to the training set\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Determine the scores for the model for both train and validation sets\n",
    "# score_train = knn_model.score(X_train, y_train)\n",
    "# score_test = knn_model.score(X_val, y_val)\n",
    "\n",
    "# # Use the model to predict on the validation set\n",
    "# knn_y_pred = knn_model.predict(X_val)\n",
    "\n",
    "# # Display the model metrics using the score_model function\n",
    "# score_model(y_val, knn_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEURAL NETWORK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch - MLPClasssifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(64,), (64, 32), (128, 64)],  # Different hidden layer architectures\n",
    "#     'activation': ['relu', 'tanh'],                     # Activation functions\n",
    "#     'solver': ['adam', 'sgd'],                          # Optimizers\n",
    "#     'alpha': [0.0001, 0.001, 0.01],                     # L2 regularization (alpha)\n",
    "#     'learning_rate_init': [0.001, 0.01, 0.1],           # Learning rates\n",
    "# }\n",
    "\n",
    "# # Create the MLPClassifier model\n",
    "# mlp = MLPClassifier(max_iter=200, random_state=42)  # Keeping max_iter constant at 200\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=mlp,\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=5,  # 5-fold cross-validation\n",
    "#                            scoring='f1_macro',  # Evaluation metric\n",
    "#                            verbose=2,           # Display progress logs\n",
    "#                            n_jobs=-1)           # Use all available processors\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score of 0.415\n",
    "\n",
    "# Create the model\n",
    "mlpc_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility\n",
    "\n",
    "# Fit the model to the training set\n",
    "mlpc_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = mlpc_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = mlpc_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "mplc_y_pred = mlpc_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, mplc_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Random Forest</a> -> (overfits) <br/>\n",
    "Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.379\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = rf_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = rf_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "rf_y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, rf_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight = np.sum(y_train == 6) / np.sum(y_train != 6)\n",
    "\n",
    "# score = 0\n",
    "# score_settings = \"\"\n",
    "\n",
    "# for x in range(1,20):\n",
    "#     for y in range(50, 151, 10):\n",
    "#         for z in np.arange(0, 1.1, 0.1):\n",
    "#             xgb_model = xgb.XGBClassifier(\n",
    "#                 n_estimators=y,  # Number of trees\n",
    "#                 learning_rate=z,  # Step size shrinkage\n",
    "#                 max_depth=x,       # Maximum depth of a tree\n",
    "#                 random_state=42,   # For reproducibility\n",
    "#                 use_label_encoder=False,  # Avoid warning for encoding\n",
    "#                 eval_metric='mlogloss',    # Evaluation metric for multi-class classification\n",
    "#                 scale_pos_weight = pos_weight\n",
    "#             )\n",
    "#             xgb_model.fit(X_train, y_train)\n",
    "#             xgb_y_pred = xgb_model.predict(X_val)\n",
    "#             f1 = f1_score(y_val, xgb_y_pred, average=\"macro\")\n",
    "\n",
    "#             if f1 > score:\n",
    "#                 score = f1\n",
    "#                 score_settings = \"max_depth: \" + str(x) + \" | n_estimators: \" + str(y) + \" | lr: \" + str(z)\n",
    "\n",
    "# print(score)\n",
    "# print(score_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://xgboost.readthedocs.io/en/stable/tutorials/index.html\">XGBoost</a> -> (tends to overfit): <br/>\n",
    "Also using decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.442\n",
    "# max_depth = 19, n_estimators = 150, lr = 0.6 -> overfitting\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 min = max_depth = 6 - .402 f1\n",
    "gbdt_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Number of boosting stages\n",
    "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
    "    max_depth=6,            # Limits depth of each tree to prevent overfitting\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = gbdt_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = gbdt_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "gbdt_y_pred = gbdt_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, gbdt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.425 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=xgb.XGBClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hidden_layer_sizes=(13,), max_iter=500, random_state=42) - 0.395 (no overfit) 12m 58s\n",
    "# (hidden_layer_sizes=(15,), max_iter=500, random_state=42) - 0.407 (no overfit) 11m 34s\n",
    "# (hidden_layer_sizes=(20,), max_iter=500, random_state=42) - 0.407 (no overfit) 15m 20s\n",
    "# (hidden_layer_sizes=(10,), max_iter=500, random_state=42) - 0.389 (no overfit) 4m 6s\n",
    "# (hidden_layer_sizes=(10,), max_iter=1000, random_state=42) - 0.389 (no overfit) 4m 8s\n",
    "base_model = MLPClassifier(hidden_layer_sizes=(18,), max_iter=500, random_state=42)\n",
    "bagging_model = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - LR -> XGB -> MLPC\n",
    "# 0.425 - LR -> MLP -> XGB w/ a 0.0099 difference in scores (37m)\n",
    "# 0.410 - MLP -> XGB -> GBC w/ a 0.011 difference in scores (93m 35s)\n",
    "\n",
    "base_models = [\n",
    "    ('mlpc', MLPClassifier()),\n",
    "    ('xgb', xgb.XGBClassifier() )\n",
    "]\n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=GradientBoostingClassifier())\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_y_pred_f1   = f1_score(y_val, lr_y_pred, average='macro')\n",
    "# dt_y_pred_f1   = f1_score(y_val, dt_y_pred, average='macro')\n",
    "# knn_y_pred_f1  = f1_score(y_val, knn_y_pred, average='macro')\n",
    "mplc_y_pred_f1 = f1_score(y_val, mplc_y_pred, average='macro')\n",
    "# rf_y_pred_f1   = f1_score(y_val, rf_y_pred, average='macro')\n",
    "xgb_y_pred_f1  = f1_score(y_val, xgb_y_pred, average='macro')\n",
    "gbdt_y_pred_f1 = f1_score(y_val, gbdt_y_pred, average='macro')\n",
    "\n",
    "# f1_score(y_actual, y_predicted, average='macro')\n",
    "\n",
    "# Assign weights based on F1 scores\n",
    "#weights = [lr_y_pred_f1, dt_y_pred_f1, knn_y_pred_f1, mplc_y_pred_f1, rf_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "weights = [mplc_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "\n",
    "# Make weighted predictions\n",
    "# lr_probs    = lr_model.predict_proba(X_val)[:, 1]\n",
    "# dt_probs    = decision_tree.predict_proba(X_val)[:, 1]\n",
    "# knn_probs   = knn_model.predict_proba(X_val)[:, 1]\n",
    "mplc_probs  = mlpc_model.predict_proba(X_val)[:, 1]\n",
    "# rf_probs    = rf_model.predict_proba(X_val)[:, 1]\n",
    "xgb_probs   = xgb_model.predict_proba(X_val)[:, 1]\n",
    "gbdt_probs  = gbdt_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Aggregate predictions using weights\n",
    "weighted_probs = (\n",
    "                    # weights[0] * lr_probs +\n",
    "                #   weights[1] * dt_probs +\n",
    "                #   weights[2] * knn_probs +\n",
    "                  weights[0] * mplc_probs + \n",
    "                #   weights[4] * rf_probs + \n",
    "                  weights[1] * xgb_probs + \n",
    "                  weights[2] * gbdt_probs)\n",
    "\n",
    "# Final predictions (threshold = 0.5)\n",
    "final_predictions = (weighted_probs >= 0.2).astype(int)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "final_f1 = f1_score(y_val, final_predictions, average='macro')\n",
    "print(f\"Weighted Ensemble F1 Score: {final_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kaggle\">\n",
    "\n",
    "## 11. Kaggle Submission\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "# y_pred_test = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode the prediction labels back to their original values\n",
    "# decoded_labels = label_encoder.inverse_transform(y_pred_test)\n",
    "# decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the prediction values with their claim identifiers into a dataframe\n",
    "# kaggle_submission = pd.DataFrame({\"Claim Identifier\": test_data.index, \"Claim Injury Type\":decoded_labels})\n",
    "# kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the resulting dataframe into a csv file named \"Kaggle_submission.csv\"\n",
    "# this will be found in the directory the file is currently running from\n",
    "# if a file exists with the same name, it will overwrite it with the new output.\n",
    "# kaggle_submission.to_csv(\"Kaggle_Submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
