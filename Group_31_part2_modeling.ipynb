{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group 31** <br>\n",
    "* Ana Margarida Valente, nr 20240936\n",
    "* Eduardo Mendes, nr 20240850\n",
    "* Julia Karpienia, nr 20240514\n",
    "* Marta Boavida, nr 20240519\n",
    "* Victoria Goon, nr 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Import Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) #Show all columns\n",
    "\n",
    "## Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets that you got from the notebook group_31_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If u want to use standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_encoded_std.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"validation_encoded_std.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"test_encoded_std.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If u want to use minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"train_encoded_minmax.csv\", low_memory=False)\n",
    "# validation_data = pd.read_csv(\"validation_encoded_minmax.csv\", low_memory=False)\n",
    "# test_data = pd.read_csv(\"test_encoded_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "validation_data = validation_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making the copy of the csv file (each time when you want to try new model (with different feature combinations) you need to make a new copy (it \"resets\" the dataset to default option (with all variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_c = train_data.copy()\n",
    "validation_data_c = validation_data.copy()\n",
    "test_data_c = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_c.drop('Claim Injury Type', axis = 1)\n",
    "y_train = train_data_c['Claim Injury Type']\n",
    "\n",
    "X_val = validation_data_c.drop('Claim Injury Type', axis = 1)\n",
    "y_val = validation_data_c['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WCIO Part of Body_cat</th>\n",
       "      <th>WCIO Nature of Injury Code_cat</th>\n",
       "      <th>WCIO Cause of Injury_cat</th>\n",
       "      <th>Industry Code_21</th>\n",
       "      <th>Industry Code_22</th>\n",
       "      <th>Industry Code_23</th>\n",
       "      <th>Industry Code_31</th>\n",
       "      <th>Industry Code_32</th>\n",
       "      <th>Industry Code_33</th>\n",
       "      <th>Industry Code_42</th>\n",
       "      <th>...</th>\n",
       "      <th>WCIO Part Of Body Code_99</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Between Accident_Assembly</th>\n",
       "      <th>Days Between Accident_C2</th>\n",
       "      <th>Industry_Avg_Weekly_Wage</th>\n",
       "      <th>Claim Count by Carrier</th>\n",
       "      <th>Claim Count by Gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5935707</th>\n",
       "      <td>Multiple Body Parts</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Strain or Injury By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.283050</td>\n",
       "      <td>0.597492</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>1.493541</td>\n",
       "      <td>-0.118281</td>\n",
       "      <td>-0.127430</td>\n",
       "      <td>-0.303680</td>\n",
       "      <td>-1.538812</td>\n",
       "      <td>-1.204980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868764</th>\n",
       "      <td>Lower Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Strain or Injury By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.431636</td>\n",
       "      <td>-0.208755</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.005048</td>\n",
       "      <td>-0.123253</td>\n",
       "      <td>-0.131062</td>\n",
       "      <td>1.783977</td>\n",
       "      <td>0.982451</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986945</th>\n",
       "      <td>Head</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Struck or Injured By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.257718</td>\n",
       "      <td>1.264815</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.005048</td>\n",
       "      <td>-0.123253</td>\n",
       "      <td>-0.131062</td>\n",
       "      <td>-0.303680</td>\n",
       "      <td>-0.908064</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665055</th>\n",
       "      <td>Trunk</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Struck or Injured By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.209680</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-1.004108</td>\n",
       "      <td>-0.085134</td>\n",
       "      <td>-0.091109</td>\n",
       "      <td>-0.303680</td>\n",
       "      <td>0.982451</td>\n",
       "      <td>-1.204980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595404</th>\n",
       "      <td>Upper Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Miscellaneous Causes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402611</td>\n",
       "      <td>1.728550</td>\n",
       "      <td>0.243009</td>\n",
       "      <td>-0.504578</td>\n",
       "      <td>-0.116624</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>1.756214</td>\n",
       "      <td>-0.787763</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 WCIO Part of Body_cat WCIO Nature of Injury Code_cat  \\\n",
       "Claim Identifier                                                        \n",
       "5935707            Multiple Body Parts                Specific Injury   \n",
       "5868764              Lower Extremities                Specific Injury   \n",
       "5986945                           Head                Specific Injury   \n",
       "5665055                          Trunk                Specific Injury   \n",
       "5595404              Upper Extremities                Specific Injury   \n",
       "\n",
       "                 WCIO Cause of Injury_cat  Industry Code_21  Industry Code_22  \\\n",
       "Claim Identifier                                                                \n",
       "5935707               Strain or Injury By               0.0               0.0   \n",
       "5868764               Strain or Injury By               0.0               0.0   \n",
       "5986945              Struck or Injured By               0.0               0.0   \n",
       "5665055              Struck or Injured By               0.0               0.0   \n",
       "5595404              Miscellaneous Causes               0.0               0.0   \n",
       "\n",
       "                  Industry Code_23  Industry Code_31  Industry Code_32  \\\n",
       "Claim Identifier                                                         \n",
       "5935707                        0.0               0.0               0.0   \n",
       "5868764                        0.0               0.0               0.0   \n",
       "5986945                        0.0               0.0               0.0   \n",
       "5665055                        0.0               0.0               0.0   \n",
       "5595404                        0.0               0.0               0.0   \n",
       "\n",
       "                  Industry Code_33  Industry Code_42  ...  \\\n",
       "Claim Identifier                                      ...   \n",
       "5935707                        0.0               0.0  ...   \n",
       "5868764                        0.0               0.0  ...   \n",
       "5986945                        0.0               0.0  ...   \n",
       "5665055                        0.0               0.0  ...   \n",
       "5595404                        0.0               0.0  ...   \n",
       "\n",
       "                  WCIO Part Of Body Code_99  Age at Injury  \\\n",
       "Claim Identifier                                             \n",
       "5935707                                 0.0       1.283050   \n",
       "5868764                                 0.0      -1.431636   \n",
       "5986945                                 0.0      -0.257718   \n",
       "5665055                                 0.0       1.209680   \n",
       "5595404                                 0.0       0.402611   \n",
       "\n",
       "                  Average Weekly Wage  IME-4 Count  Number of Dependents  \\\n",
       "Claim Identifier                                                           \n",
       "5935707                      0.597492    -0.420035              1.493541   \n",
       "5868764                     -0.208755    -0.420035             -0.005048   \n",
       "5986945                      1.264815    -0.420035             -0.005048   \n",
       "5665055                     -0.535991    -0.420035             -1.004108   \n",
       "5595404                      1.728550     0.243009             -0.504578   \n",
       "\n",
       "                  Days Between Accident_Assembly  Days Between Accident_C2  \\\n",
       "Claim Identifier                                                             \n",
       "5935707                                -0.118281                 -0.127430   \n",
       "5868764                                -0.123253                 -0.131062   \n",
       "5986945                                -0.123253                 -0.131062   \n",
       "5665055                                -0.085134                 -0.091109   \n",
       "5595404                                -0.116624                  0.074153   \n",
       "\n",
       "                  Industry_Avg_Weekly_Wage  Claim Count by Carrier  \\\n",
       "Claim Identifier                                                     \n",
       "5935707                          -0.303680               -1.538812   \n",
       "5868764                           1.783977                0.982451   \n",
       "5986945                          -0.303680               -0.908064   \n",
       "5665055                          -0.303680                0.982451   \n",
       "5595404                           1.756214               -0.787763   \n",
       "\n",
       "                  Claim Count by Gender  \n",
       "Claim Identifier                         \n",
       "5935707                       -1.204980  \n",
       "5868764                        0.829889  \n",
       "5986945                        0.829889  \n",
       "5665055                       -1.204980  \n",
       "5595404                        0.829889  \n",
       "\n",
       "[5 rows x 323 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WCIO Part of Body_cat</th>\n",
       "      <th>WCIO Nature of Injury Code_cat</th>\n",
       "      <th>WCIO Cause of Injury_cat</th>\n",
       "      <th>Industry Code_21</th>\n",
       "      <th>Industry Code_22</th>\n",
       "      <th>Industry Code_23</th>\n",
       "      <th>Industry Code_31</th>\n",
       "      <th>Industry Code_32</th>\n",
       "      <th>Industry Code_33</th>\n",
       "      <th>Industry Code_42</th>\n",
       "      <th>...</th>\n",
       "      <th>WCIO Part Of Body Code_99</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Between Accident_Assembly</th>\n",
       "      <th>Days Between Accident_C2</th>\n",
       "      <th>Industry_Avg_Weekly_Wage</th>\n",
       "      <th>Claim Count by Carrier</th>\n",
       "      <th>Claim Count by Gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5517094</th>\n",
       "      <td>Lower Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Fall, Slip or Trip Injury</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.477828</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-1.004108</td>\n",
       "      <td>-0.131540</td>\n",
       "      <td>-0.140142</td>\n",
       "      <td>-0.303680</td>\n",
       "      <td>0.982451</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133770</th>\n",
       "      <td>Upper Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Struck or Injured By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.331088</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>0.994011</td>\n",
       "      <td>-0.134855</td>\n",
       "      <td>-0.143774</td>\n",
       "      <td>-0.801528</td>\n",
       "      <td>0.982451</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741413</th>\n",
       "      <td>Trunk</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Strain or Injury By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062940</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-1.004108</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.140142</td>\n",
       "      <td>1.783977</td>\n",
       "      <td>0.982451</td>\n",
       "      <td>0.829889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6082466</th>\n",
       "      <td>Upper Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Struck or Injured By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.404458</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.005048</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>0.028751</td>\n",
       "      <td>-0.904869</td>\n",
       "      <td>-0.787763</td>\n",
       "      <td>-1.204980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086244</th>\n",
       "      <td>Lower Extremities</td>\n",
       "      <td>Specific Injury</td>\n",
       "      <td>Strain or Injury By</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.578376</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-1.004108</td>\n",
       "      <td>-0.131540</td>\n",
       "      <td>-0.140142</td>\n",
       "      <td>-0.904869</td>\n",
       "      <td>-0.787763</td>\n",
       "      <td>-1.204980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 WCIO Part of Body_cat WCIO Nature of Injury Code_cat  \\\n",
       "Claim Identifier                                                        \n",
       "5517094              Lower Extremities                Specific Injury   \n",
       "6133770              Upper Extremities                Specific Injury   \n",
       "5741413                          Trunk                Specific Injury   \n",
       "6082466              Upper Extremities                Specific Injury   \n",
       "6086244              Lower Extremities                Specific Injury   \n",
       "\n",
       "                   WCIO Cause of Injury_cat  Industry Code_21  \\\n",
       "Claim Identifier                                                \n",
       "5517094           Fall, Slip or Trip Injury               0.0   \n",
       "6133770                Struck or Injured By               0.0   \n",
       "5741413                 Strain or Injury By               0.0   \n",
       "6082466                Struck or Injured By               0.0   \n",
       "6086244                 Strain or Injury By               0.0   \n",
       "\n",
       "                  Industry Code_22  Industry Code_23  Industry Code_31  \\\n",
       "Claim Identifier                                                         \n",
       "5517094                        0.0               0.0               0.0   \n",
       "6133770                        0.0               0.0               0.0   \n",
       "5741413                        0.0               0.0               0.0   \n",
       "6082466                        0.0               0.0               0.0   \n",
       "6086244                        0.0               0.0               0.0   \n",
       "\n",
       "                  Industry Code_32  Industry Code_33  Industry Code_42  ...  \\\n",
       "Claim Identifier                                                        ...   \n",
       "5517094                        0.0               0.0               0.0  ...   \n",
       "6133770                        0.0               0.0               0.0  ...   \n",
       "5741413                        0.0               0.0               0.0  ...   \n",
       "6082466                        0.0               0.0               0.0  ...   \n",
       "6086244                        0.0               0.0               0.0  ...   \n",
       "\n",
       "                  WCIO Part Of Body Code_99  Age at Injury  \\\n",
       "Claim Identifier                                             \n",
       "5517094                                 0.0      -0.477828   \n",
       "6133770                                 0.0      -0.331088   \n",
       "5741413                                 0.0       1.062940   \n",
       "6082466                                 0.0      -0.404458   \n",
       "6086244                                 0.0      -1.578376   \n",
       "\n",
       "                  Average Weekly Wage  IME-4 Count  Number of Dependents  \\\n",
       "Claim Identifier                                                           \n",
       "5517094                     -0.535991    -0.420035             -1.004108   \n",
       "6133770                     -0.535991    -0.420035              0.994011   \n",
       "5741413                     -0.535991    -0.420035             -1.004108   \n",
       "6082466                     -0.535991    -0.420035             -0.005048   \n",
       "6086244                     -0.535991    -0.420035             -1.004108   \n",
       "\n",
       "                  Days Between Accident_Assembly  Days Between Accident_C2  \\\n",
       "Claim Identifier                                                             \n",
       "5517094                                -0.131540                 -0.140142   \n",
       "6133770                                -0.134855                 -0.143774   \n",
       "5741413                                -0.129883                 -0.140142   \n",
       "6082466                                 0.027567                  0.028751   \n",
       "6086244                                -0.131540                 -0.140142   \n",
       "\n",
       "                  Industry_Avg_Weekly_Wage  Claim Count by Carrier  \\\n",
       "Claim Identifier                                                     \n",
       "5517094                          -0.303680                0.982451   \n",
       "6133770                          -0.801528                0.982451   \n",
       "5741413                           1.783977                0.982451   \n",
       "6082466                          -0.904869               -0.787763   \n",
       "6086244                          -0.904869               -0.787763   \n",
       "\n",
       "                  Claim Count by Gender  \n",
       "Claim Identifier                         \n",
       "5517094                        0.829889  \n",
       "6133770                        0.829889  \n",
       "5741413                        0.829889  \n",
       "6082466                       -1.204980  \n",
       "6086244                       -1.204980  \n",
       "\n",
       "[5 rows x 323 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Identifier\n",
       "5517094    2. NON-COMP\n",
       "6133770    2. NON-COMP\n",
       "5741413    2. NON-COMP\n",
       "6082466    2. NON-COMP\n",
       "6086244    3. MED ONLY\n",
       "Name: Claim Injury Type, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encode Target Variable\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as here to be able to interpret the results properly when a model is tested.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_val_unencoded = y_train.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment variables that you want to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop = [\n",
    "    #  'Age at Injury', \n",
    "      # 'Alternative Dispute Resolution',\n",
    "    #  'Attorney/Representative',\n",
    "    #  'Average Weekly Wage',\n",
    "      # 'Carrier Type',\n",
    "      # 'County of Injury', \n",
    "    #  'COVID-19 Indicator', \n",
    "    #   'District Name', \n",
    "     # 'Gender',\n",
    "    #   'IME-4 Count', \n",
    "      # 'Industry Code', \n",
    "    #   'Medical Fee Region',\n",
    "   # 'WCIO Cause of Injury Code', \n",
    "   # 'WCIO Nature of Injury Code',\n",
    "   # 'WCIO Part Of Body Code', \n",
    "       'Number of Dependents', \n",
    "      # 'zip_code_cat',\n",
    "    #    'First Hearing Date Binary',\n",
    "    #   'C-2 Date Bin', \n",
    "    #   'C-3 Date Bin',\n",
    "    # 'Days Between Accident_Assembly', \n",
    "       'Days Between Accident_C2',\n",
    "      #  'Season_of_Accident', \n",
    "      #  'Industry_Avg_Weekly_Wage'\n",
    "      #  'Claim Count by Carrier',\n",
    "      #  'Claim Count by Gender'\n",
    "       ]\n",
    "\n",
    "# Identify columns to drop for each dataset\n",
    "columns_to_drop_train = [col for col in X_train.columns if any(col.startswith(var) for var in variables_to_drop)]\n",
    "columns_to_drop_val = [col for col in X_val.columns if any(col.startswith(var) for var in variables_to_drop)]\n",
    "columns_to_drop_test = [col for col in test_data_c.columns if any(col.startswith(var) for var in variables_to_drop)]\n",
    "\n",
    "# Drop the identified columns\n",
    "X_train = X_train.drop(columns=columns_to_drop_train)\n",
    "X_val = X_val.drop(columns=columns_to_drop_val)\n",
    "test_data_c = test_data_c.drop(columns=columns_to_drop_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the encoded variables back to the x set\n",
    "# training_data_undersampled = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_classes = {}\n",
    "# for x in range(0,8):\n",
    "#     if x != 6:\n",
    "#         majority_classes[x] = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == x]\n",
    "\n",
    "# minority_class = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == 6]\n",
    "\n",
    "# size = int(len(minority_class) + (len(minority_class) * 2))\n",
    "\n",
    "# print(size)\n",
    "\n",
    "# # Perform undersampling\n",
    "# undersampled_majority_0 = majority_classes[0].sample(n=size, random_state=42)\n",
    "# undersampled_majority_1 = majority_classes[1].sample(n=size, random_state=42)\n",
    "# undersampled_majority_2 = majority_classes[2].sample(n=size, random_state=42)\n",
    "# undersampled_majority_3 = majority_classes[3].sample(n=size, random_state=42)\n",
    "# undersampled_majority_4 = majority_classes[4].sample(n=size, random_state=42)\n",
    "# undersampled_majority_5 = majority_classes[5].sample(n=size, random_state=42)\n",
    "# undersampled_majority_7 = majority_classes[7].sample(n=size, random_state=42)\n",
    "# # undersampled_majority.head()\n",
    "# balanced_data = pd.concat([undersampled_majority_0, undersampled_majority_1, undersampled_majority_2, \n",
    "#                            undersampled_majority_3, undersampled_majority_4, undersampled_majority_5, \n",
    "#                            minority_class, undersampled_majority_7])\n",
    "\n",
    "# # Separate features and target\n",
    "# X_train = balanced_data.drop(columns='encoded_target')\n",
    "# y_train = balanced_data['encoded_target']\n",
    "\n",
    "# # Check class distribution after undersampling\n",
    "# print(\"Class distribution after undersampling:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize SMOTE\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# # Fit and resample the dataset\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(\"Class distribution after oversampling:\", y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "## 2. Model\n",
    "</a>\n",
    "\n",
    "Type of Problem <br/>\n",
    "The type of problem to be solved is a multiclassification problem where the output is between 8 different choices. We will use a simple Logistical Regression model set to be able to compute multiple classes.<br/>\n",
    "<br/>\n",
    "Metric used:<br/>\n",
    "As a classification problem, we observed the following metrics to determine the effectiveness of our model:\n",
    " - accuracy\n",
    " - precision\n",
    " - recall\n",
    " - f1 score\n",
    "\n",
    " Each point is measured in a different and observing them all allows us to get an accurate view of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs'], 'class_weight': [None, 'balanced']}\n",
    "# grid_search = GridSearchCV(LogisticRegression(multi_class='multinomial', random_state=42), param_grid, cv=5, scoring='f1_macro')\n",
    "# grid_search.fit(X_train_std_scaler_encoded, Y_train_encoded_df)\n",
    "\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.379\n",
    "# (oversampling) - 0.323 - 40s\n",
    "\n",
    "# Create the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = lr_model.score(X_train, y_train)\n",
    "score_test = lr_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "lr_y_pred = lr_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, lr_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Initialize the Decision Tree Classifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],                          \n",
    "#     'splitter': ['best', 'random'],                             \n",
    "#     'max_depth': [None, 10, 20, 30],                           \n",
    "#     'min_samples_split': [2, 5, 10],                            \n",
    "#     'min_samples_leaf': [1, 2, 4],                              \n",
    "#     'max_features': [None, 'sqrt', 'log2'],                    \n",
    "#     'max_leaf_nodes': [None, 10, 20, 30],                       \n",
    "#     'min_impurity_decrease': [0.0, 0.1, 0.2]                   \n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV:\n",
    "# grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# #Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# #Best Score: 0.7769977245887005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.366\n",
    "# (oversampling) - 0.343 - 21s\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  \n",
    "    max_depth=10, \n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease= 0.0,\n",
    "    min_samples_leaf= 1,\n",
    "    min_samples_split=2,\n",
    "    splitter='best',  \n",
    "    random_state=42    \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = decision_tree.score(X_train, y_train)\n",
    "score_test = decision_tree.score(X_val, y_val)\n",
    "\n",
    "# Make predictions\n",
    "dt_y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, dt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for Randomized Search\n",
    "# param_distributions = {\n",
    "#     'n_neighbors' : [5,10], \n",
    "#     'leaf_size': [30, 50],                                  \n",
    "#     'metric': ['euclidean', 'manhattan'],          \n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV with KNN classifier\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=KNeighborsClassifier(),\n",
    "#     param_distributions=param_distributions,\n",
    "#     n_iter=5,                                     \n",
    "#     cv=2,                                          \n",
    "#     scoring='f1_macro',                            \n",
    "#     verbose=2,                                     \n",
    "#     random_state=42,                               \n",
    "#     n_jobs=-1                                      \n",
    "# )\n",
    "\n",
    "# # Fit the random search to the training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# # Best Parameters: {'n_neighbors': 5, 'metric': 'euclidean', 'leaf_size': 30}\n",
    "# # Best Score: 0.328445945439427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - KNN<br/>\n",
    "- KNN is not appropriate for too large datasets. Too computational expensive due to memorization requirements.\n",
    "- KNN takes too long to process due to our large dataset so will be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.334\n",
    "# # Initialize the KNN Classifier\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5, leaf_size=30, metric='euclidean')  \n",
    "\n",
    "# # Train the model\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Determine the scores for the model for both train and validation sets\n",
    "# score_train = knn_model.score(X_train, y_train)\n",
    "# score_test = knn_model.score(X_val, y_val)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = knn_model.predict(X_val)\n",
    "\n",
    "# # Display the model metrics using the score_model function\n",
    "# score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Gaussian Naive Bayes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumes normality, independence, Homogeneity of Variance (Homoscedasticity): \n",
    "- Will be commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the model\n",
    "# model = GaussianNB()\n",
    "\n",
    "# # fit the model to the training set\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # determine the scores for the model for both train and validation\n",
    "# score_train = model.score(X_train, y_train)\n",
    "# score_test = model.score(X_val, y_val)\n",
    "\n",
    "# # use model to predict on validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "# # display the model metrics\n",
    "# score_model(y_val, y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEURAL NETWORK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch - MLPClasssifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {  \n",
    "#     'hidden_layer_sizes': [\n",
    "#         (50),        # Larger single-layer model\n",
    "#         (50, 30),      # Moderate two-layer model\n",
    "#         (100, 50),     # Larger two-layer model\n",
    "#         (128, 64, 32)  # Complex three-layer model\n",
    "#     ],\n",
    "#     'activation': ['relu', 'logistic'],                \n",
    "#     'solver': ['adam', 'sgd'],                     \n",
    "#     'alpha': [0.0001, 0.001],                       \n",
    "#     'learning_rate': ['adaptive', 'invscaling']          \n",
    "# }\n",
    "\n",
    "# # Initialize the Neural Network model\n",
    "# mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# # Initialize Random Search for hyperparameter tuning\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=mlp,\n",
    "#     param_distributions=param_grid,  # Using param_distributions for randomized search\n",
    "#     n_iter=10,  # Number of random combinations to try\n",
    "#     cv=2,  # 3-fold cross-validation\n",
    "#     scoring='f1_macro',  # Evaluation metric\n",
    "#     verbose=2,  # Display progress logs\n",
    "#     n_jobs=-1,  # Use all available processors for parallel computation\n",
    "#     random_state=42  # For reproducibility\n",
    "# )\n",
    "\n",
    "# # Fit the randomized search to the training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# \n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# # Best Parameters: {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 30), 'alpha': 0.001, 'activation': 'logistic'}\n",
    "# # Best Score: 0.4192846184298069\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL - Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.55\n",
      "[2. NON-COMP]:     \t0.9\n",
      "[3. MED ONLY]:     \t0.15\n",
      "[4. TEMPORARY]:     \t0.77\n",
      "[5. PPD SCH LOSS]:     \t0.61\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.32\n",
      "\n",
      "Macro f1: 0.412\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.7896692782013076\n",
      "Test Score: 0.7751614326860541\n",
      "Difference: 0.014507845515253526\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7751614326860541\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.7\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.48\n",
      "[4. TEMPORARY]:     \t0.69\n",
      "[5. PPD SCH LOSS]:     \t0.69\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.4\n",
      "\n",
      "Macro precision: 0.476\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.45\n",
      "[2. NON-COMP]:     \t0.95\n",
      "[3. MED ONLY]:     \t0.09\n",
      "[4. TEMPORARY]:     \t0.88\n",
      "[5. PPD SCH LOSS]:     \t0.55\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.27\n",
      "\n",
      "Macro recall: 0.398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#UPDATED VERSION:\n",
    "\n",
    "# # Initialize the Neural Network model\n",
    "model = MLPClassifier(hidden_layer_sizes=(50, 30),  \n",
    "                      activation='logistic',        \n",
    "                      solver='adam',               \n",
    "                      alpha=0.0001,                \n",
    "                      learning_rate='adaptive',    \n",
    "                      max_iter=200,                \n",
    "                      random_state=42)             \n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.432\n",
    "- using StandardScaler and with deleting: Number of Dependents and Days Between Acciden_C2 i got score: 0.412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score of 0.415\n",
    "# (oversampling) - 0.407 - (0.11 diff in scores) - 12m 3s \n",
    "\n",
    "# Initialize the Neural Network model\n",
    "mlpc_model = MLPClassifier(hidden_layer_sizes=(64, 32),  \n",
    "                      activation='relu',           \n",
    "                      solver='adam',               \n",
    "                      alpha=0.0001,                \n",
    "                      learning_rate_init=0.001,    \n",
    "                      max_iter=200,                \n",
    "                      random_state=42)             \n",
    "\n",
    "# Train the model\n",
    "mlpc_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = mlpc_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = mlpc_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "mplc_y_pred = mlpc_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, mplc_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Random Forest</a> -> (overfits) <br/>\n",
    "Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.55\n",
      "[2. NON-COMP]:     \t0.9\n",
      "[3. MED ONLY]:     \t0.12\n",
      "[4. TEMPORARY]:     \t0.77\n",
      "[5. PPD SCH LOSS]:     \t0.58\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.09\n",
      "\n",
      "Macro f1: 0.378\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.9999447205624431\n",
      "Test Score: 0.7760034377032425\n",
      "Difference: 0.22394128285920056\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7760034377032425\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.73\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.5\n",
      "[4. TEMPORARY]:     \t0.68\n",
      "[5. PPD SCH LOSS]:     \t0.71\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.54\n",
      "\n",
      "Macro precision: 0.501\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.45\n",
      "[2. NON-COMP]:     \t0.96\n",
      "[3. MED ONLY]:     \t0.07\n",
      "[4. TEMPORARY]:     \t0.89\n",
      "[5. PPD SCH LOSS]:     \t0.49\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.05\n",
      "\n",
      "Macro recall: 0.364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.379\n",
    "# (oversampling) - 0.433 (overfitting w/ 0.23 diff) - 6m 5s\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = rf_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = rf_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "rf_y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, rf_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.433\n",
    "- using StandardScaler and with deleting: Number of Dependents and Days Between Acciden_C2 i got score 0.378 (2 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight = np.sum(y_train == 6) / np.sum(y_train != 6)\n",
    "\n",
    "# score = 0\n",
    "# score_settings = \"\"\n",
    "\n",
    "# for x in range(1,20):\n",
    "#     for y in range(50, 151, 10):\n",
    "#         for z in np.arange(0, 1.1, 0.1):\n",
    "#             xgb_model = xgb.XGBClassifier(\n",
    "#                 n_estimators=y,  # Number of trees\n",
    "#                 learning_rate=z,  # Step size shrinkage\n",
    "#                 max_depth=x,       # Maximum depth of a tree\n",
    "#                 random_state=42,   # For reproducibility\n",
    "#                 use_label_encoder=False,  # Avoid warning for encoding\n",
    "#                 eval_metric='mlogloss',    # Evaluation metric for multi-class classification\n",
    "#                 scale_pos_weight = pos_weight\n",
    "#             )\n",
    "#             xgb_model.fit(X_train, y_train)\n",
    "#             xgb_y_pred = xgb_model.predict(X_val)\n",
    "#             f1 = f1_score(y_val, xgb_y_pred, average=\"macro\")\n",
    "\n",
    "#             if f1 > score:\n",
    "#                 score = f1\n",
    "#                 score_settings = \"max_depth: \" + str(x) + \" | n_estimators: \" + str(y) + \" | lr: \" + str(z)\n",
    "\n",
    "# print(score)\n",
    "# print(score_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://xgboost.readthedocs.io/en/stable/tutorials/index.html\">XGBoost</a> -> (tends to overfit): <br/>\n",
    "Also using decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.58\n",
      "[2. NON-COMP]:     \t0.91\n",
      "[3. MED ONLY]:     \t0.13\n",
      "[4. TEMPORARY]:     \t0.78\n",
      "[5. PPD SCH LOSS]:     \t0.62\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.39\n",
      "\n",
      "Macro f1: 0.426\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.7979662192382494\n",
      "Test Score: 0.785671978073028\n",
      "Difference: 0.012294241165221376\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.785671978073028\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.72\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.51\n",
      "[4. TEMPORARY]:     \t0.71\n",
      "[5. PPD SCH LOSS]:     \t0.68\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.6\n",
      "\n",
      "Macro precision: 0.509\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.49\n",
      "[2. NON-COMP]:     \t0.97\n",
      "[3. MED ONLY]:     \t0.07\n",
      "[4. TEMPORARY]:     \t0.88\n",
      "[5. PPD SCH LOSS]:     \t0.57\n",
      "[6. PPD NSL]:     \t0.0\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.29\n",
      "\n",
      "Macro recall: 0.408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.442\n",
    "# (oversampling) 0.453 (overfit by 0.10 diff) 4m 18s\n",
    "\n",
    "# max_depth = 19, n_estimators = 150, lr = 0.6 -> overfitting\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting: Number of Dependents i got score 0.432\n",
    "- using StandardScaler and with deleting: Number of Dependents and Days Between Accident_Assembly i got score 0.427 \n",
    "- using StandardScaler and with deleting: Number of Dependents and Days Between Acciden_C2 i got score 0.426\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 min = max_depth = 6 - .402 f1\n",
    "gbdt_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Number of boosting stages\n",
    "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
    "    max_depth=6,            # Limits depth of each tree to prevent overfitting\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = gbdt_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = gbdt_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "gbdt_y_pred = gbdt_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, gbdt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling - 0.405 - 6m 55s - overfitting by 0.29\n",
    "# 0.4 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with correct hyperparameters - 0.451 - 0.104 for overfitting\n",
    "# oversampling - 0.448 - 17m 59s - 0.107 diff for overfitting\n",
    "# 0.425 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    "), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hidden_layer_sizes=(13,), max_iter=500, random_state=42) - 0.395 (no overfit) 12m 58s\n",
    "# (hidden_layer_sizes=(15,), max_iter=500, random_state=42) - 0.407 (no overfit) 11m 34s\n",
    "# (hidden_layer_sizes=(20,), max_iter=500, random_state=42) - 0.407 (no overfit) 15m 20s\n",
    "# (hidden_layer_sizes=(10,), max_iter=500, random_state=42) - 0.389 (no overfit) 4m 6s\n",
    "# (hidden_layer_sizes=(10,), max_iter=1000, random_state=42) - 0.389 (no overfit) 4m 8s\n",
    "base_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility)\n",
    "bagging_model = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)\n",
    "# try this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.440 - LR -> XGB -> MLP w/ a 0.015 difference in scores (3m 55s)\n",
    "# 0.425 - LR -> MLP -> XGB w/ a 0.0099 difference in scores (37m)\n",
    "# 0.410 - MLP -> XGB -> GBC w/ a 0.011 difference in scores (93m 35s)\n",
    "\n",
    "base_models = [\n",
    "    ('mlpc', LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ") )\n",
    "]\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42) \n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=nn)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using StandardScaler and with deleting only Number of Dependents i got score 0.429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lr_y_pred_f1   = f1_score(y_val, lr_y_pred, average='macro')\n",
    "# # dt_y_pred_f1   = f1_score(y_val, dt_y_pred, average='macro')\n",
    "# # knn_y_pred_f1  = f1_score(y_val, knn_y_pred, average='macro')\n",
    "# mplc_y_pred_f1 = f1_score(y_val, mplc_y_pred, average='macro')\n",
    "# # rf_y_pred_f1   = f1_score(y_val, rf_y_pred, average='macro')\n",
    "# xgb_y_pred_f1  = f1_score(y_val, xgb_y_pred, average='macro')\n",
    "# gbdt_y_pred_f1 = f1_score(y_val, gbdt_y_pred, average='macro')\n",
    "\n",
    "# # f1_score(y_actual, y_predicted, average='macro')\n",
    "\n",
    "# # Assign weights based on F1 scores\n",
    "# #weights = [lr_y_pred_f1, dt_y_pred_f1, knn_y_pred_f1, mplc_y_pred_f1, rf_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = [mplc_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "\n",
    "# # Make weighted predictions\n",
    "# # lr_probs    = lr_model.predict_proba(X_val)[:, 1]\n",
    "# # dt_probs    = decision_tree.predict_proba(X_val)[:, 1]\n",
    "# # knn_probs   = knn_model.predict_proba(X_val)[:, 1]\n",
    "# mplc_probs  = mlpc_model.predict_proba(X_val)[:, 1]\n",
    "# # rf_probs    = rf_model.predict_proba(X_val)[:, 1]\n",
    "# xgb_probs   = xgb_model.predict_proba(X_val)[:, 1]\n",
    "# gbdt_probs  = gbdt_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# # Aggregate predictions using weights\n",
    "# weighted_probs = (\n",
    "#                     # weights[0] * lr_probs +\n",
    "#                 #   weights[1] * dt_probs +\n",
    "#                 #   weights[2] * knn_probs +\n",
    "#                   weights[0] * mplc_probs + \n",
    "#                 #   weights[4] * rf_probs + \n",
    "#                   weights[1] * xgb_probs + \n",
    "#                   weights[2] * gbdt_probs)\n",
    "\n",
    "# # Final predictions (threshold = 0.5)\n",
    "# final_predictions = (weighted_probs >= 0.2).astype(int)\n",
    "\n",
    "# # Evaluate the ensemble\n",
    "# final_f1 = f1_score(y_val, final_predictions, average='macro')\n",
    "# print(f\"Weighted Ensemble F1 Score: {final_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kaggle\">\n",
    "\n",
    "## 11. Kaggle Submission\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "# y_pred_test = model.predict(test_data)\n",
    "y_pred_test = xgb_model.predict(test_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the prediction labels back to their original values\n",
    "decoded_labels = label_encoder.inverse_transform(y_pred_test)\n",
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the prediction values with their claim identifiers into a dataframe\n",
    "kaggle_submission = pd.DataFrame({\"Claim Identifier\": test_data.index, \"Claim Injury Type\":decoded_labels})\n",
    "kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the resulting dataframe into a csv file named \"Kaggle_submission.csv\"\n",
    "#this will be found in the directory the file is currently running from\n",
    "#if a file exists with the same name, it will overwrite it with the new output.\n",
    "kaggle_submission.to_csv(\"Kaggle_Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
