{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group 31** <br>\n",
    "* Ana Margarida Valente, nr 20240936\n",
    "* Eduardo Mendes, nr 20240850\n",
    "* Julia Karpienia, nr 20240514\n",
    "* Marta Boavida, nr 20240519\n",
    "* Victoria Goon, nr 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Import Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import Cross Validation methods\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) #Show all columns\n",
    "\n",
    "## Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"importdatasets\">\n",
    "\n",
    "## 1. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_encoded.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"validation_encoded.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"test_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "validation_data = validation_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Claim Injury Type', axis = 1)\n",
    "y_train = train_data['Claim Injury Type']\n",
    "\n",
    "X_val = validation_data.drop('Claim Injury Type', axis = 1)\n",
    "y_val = validation_data['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First Hearing Date Binary</th>\n",
       "      <th>C-2 Date Bin</th>\n",
       "      <th>C-3 Date Bin</th>\n",
       "      <th>Carrier Type_2A. SIF</th>\n",
       "      <th>Carrier Type_3A. SELF PUBLIC</th>\n",
       "      <th>Carrier Type_4A. SELF PRIVATE</th>\n",
       "      <th>Carrier Type_5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)</th>\n",
       "      <th>Carrier Type_5C. SPECIAL FUND - POI CARRIER WCB MENANDS</th>\n",
       "      <th>Carrier Type_5D. SPECIAL FUND - UNKNOWN</th>\n",
       "      <th>County of Injury_ALLEGANY</th>\n",
       "      <th>...</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Days Between Accident_Assembly</th>\n",
       "      <th>Industry_Avg_Weekly_Wage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5935707</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217288</td>\n",
       "      <td>0.057777</td>\n",
       "      <td>0.096601</td>\n",
       "      <td>0.142603</td>\n",
       "      <td>1.283050</td>\n",
       "      <td>0.597492</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.118281</td>\n",
       "      <td>-0.303680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868764</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.057777</td>\n",
       "      <td>0.096601</td>\n",
       "      <td>0.043319</td>\n",
       "      <td>-1.431636</td>\n",
       "      <td>-0.208755</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.123253</td>\n",
       "      <td>1.783977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217288</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.193280</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>-0.257718</td>\n",
       "      <td>1.264815</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.123253</td>\n",
       "      <td>-0.303680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665055</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217288</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>0.062649</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>1.209680</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.085134</td>\n",
       "      <td>-0.303680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595404</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162145</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.193280</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>0.402611</td>\n",
       "      <td>1.728550</td>\n",
       "      <td>0.243009</td>\n",
       "      <td>-0.116624</td>\n",
       "      <td>1.756214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  First Hearing Date Binary  C-2 Date Bin  C-3 Date Bin  \\\n",
       "Claim Identifier                                                          \n",
       "5935707                                   0             1             1   \n",
       "5868764                                   0             1             0   \n",
       "5986945                                   0             1             0   \n",
       "5665055                                   0             1             1   \n",
       "5595404                                   0             1             1   \n",
       "\n",
       "                  Carrier Type_2A. SIF  Carrier Type_3A. SELF PUBLIC  \\\n",
       "Claim Identifier                                                       \n",
       "5935707                            0.0                           0.0   \n",
       "5868764                            0.0                           0.0   \n",
       "5986945                            1.0                           0.0   \n",
       "5665055                            0.0                           0.0   \n",
       "5595404                            0.0                           1.0   \n",
       "\n",
       "                  Carrier Type_4A. SELF PRIVATE  \\\n",
       "Claim Identifier                                  \n",
       "5935707                                     1.0   \n",
       "5868764                                     0.0   \n",
       "5986945                                     0.0   \n",
       "5665055                                     0.0   \n",
       "5595404                                     0.0   \n",
       "\n",
       "                  Carrier Type_5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)  \\\n",
       "Claim Identifier                                                             \n",
       "5935707                                                                0.0   \n",
       "5868764                                                                0.0   \n",
       "5986945                                                                0.0   \n",
       "5665055                                                                0.0   \n",
       "5595404                                                                0.0   \n",
       "\n",
       "                  Carrier Type_5C. SPECIAL FUND - POI CARRIER WCB MENANDS  \\\n",
       "Claim Identifier                                                            \n",
       "5935707                                                               0.0   \n",
       "5868764                                                               0.0   \n",
       "5986945                                                               0.0   \n",
       "5665055                                                               0.0   \n",
       "5595404                                                               0.0   \n",
       "\n",
       "                  Carrier Type_5D. SPECIAL FUND - UNKNOWN  \\\n",
       "Claim Identifier                                            \n",
       "5935707                                               0.0   \n",
       "5868764                                               0.0   \n",
       "5986945                                               0.0   \n",
       "5665055                                               0.0   \n",
       "5595404                                               0.0   \n",
       "\n",
       "                  County of Injury_ALLEGANY  ...  Gender  Industry Code  \\\n",
       "Claim Identifier                             ...                          \n",
       "5935707                                 0.0  ...       0       0.217288   \n",
       "5868764                                 0.0  ...       1       0.015888   \n",
       "5986945                                 0.0  ...       1       0.217288   \n",
       "5665055                                 0.0  ...       0       0.217288   \n",
       "5595404                                 0.0  ...       1       0.162145   \n",
       "\n",
       "                  WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "Claim Identifier                                                          \n",
       "5935707                            0.057777                    0.096601   \n",
       "5868764                            0.057777                    0.096601   \n",
       "5986945                            0.005654                    0.193280   \n",
       "5665055                            0.064963                    0.062649   \n",
       "5595404                            0.013888                    0.193280   \n",
       "\n",
       "                  WCIO Part Of Body Code  Age at Injury  Average Weekly Wage  \\\n",
       "Claim Identifier                                                               \n",
       "5935707                         0.142603       1.283050             0.597492   \n",
       "5868764                         0.043319      -1.431636            -0.208755   \n",
       "5986945                         0.024137      -0.257718             1.264815   \n",
       "5665055                         0.003033       1.209680            -0.535991   \n",
       "5595404                         0.019607       0.402611             1.728550   \n",
       "\n",
       "                  IME-4 Count  Days Between Accident_Assembly  \\\n",
       "Claim Identifier                                                \n",
       "5935707             -0.420035                       -0.118281   \n",
       "5868764             -0.420035                       -0.123253   \n",
       "5986945             -0.420035                       -0.123253   \n",
       "5665055             -0.420035                       -0.085134   \n",
       "5595404              0.243009                       -0.116624   \n",
       "\n",
       "                  Industry_Avg_Weekly_Wage  \n",
       "Claim Identifier                            \n",
       "5935707                          -0.303680  \n",
       "5868764                           1.783977  \n",
       "5986945                          -0.303680  \n",
       "5665055                          -0.303680  \n",
       "5595404                           1.756214  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First Hearing Date Binary</th>\n",
       "      <th>C-2 Date Bin</th>\n",
       "      <th>C-3 Date Bin</th>\n",
       "      <th>Carrier Type_2A. SIF</th>\n",
       "      <th>Carrier Type_3A. SELF PUBLIC</th>\n",
       "      <th>Carrier Type_4A. SELF PRIVATE</th>\n",
       "      <th>Carrier Type_5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)</th>\n",
       "      <th>Carrier Type_5C. SPECIAL FUND - POI CARRIER WCB MENANDS</th>\n",
       "      <th>Carrier Type_5D. SPECIAL FUND - UNKNOWN</th>\n",
       "      <th>County of Injury_ALLEGANY</th>\n",
       "      <th>...</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Days Between Accident_Assembly</th>\n",
       "      <th>Industry_Avg_Weekly_Wage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5517094</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217288</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>0.039703</td>\n",
       "      <td>0.032572</td>\n",
       "      <td>-0.477828</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.131540</td>\n",
       "      <td>-0.253801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133770</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.021574</td>\n",
       "      <td>0.021720</td>\n",
       "      <td>0.055136</td>\n",
       "      <td>-0.331088</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.134855</td>\n",
       "      <td>-0.023187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741413</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.108591</td>\n",
       "      <td>0.294255</td>\n",
       "      <td>0.089553</td>\n",
       "      <td>1.062940</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>1.980481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6082466</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077592</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>0.193280</td>\n",
       "      <td>0.055136</td>\n",
       "      <td>-0.404458</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>-0.856314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077592</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.294255</td>\n",
       "      <td>0.083570</td>\n",
       "      <td>-1.578376</td>\n",
       "      <td>-0.535991</td>\n",
       "      <td>-0.420035</td>\n",
       "      <td>-0.131540</td>\n",
       "      <td>-0.856314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  First Hearing Date Binary  C-2 Date Bin  C-3 Date Bin  \\\n",
       "Claim Identifier                                                          \n",
       "5517094                                   0             1             0   \n",
       "6133770                                   0             1             1   \n",
       "5741413                                   0             1             0   \n",
       "6082466                                   0             1             0   \n",
       "6086244                                   0             1             1   \n",
       "\n",
       "                  Carrier Type_2A. SIF  Carrier Type_3A. SELF PUBLIC  \\\n",
       "Claim Identifier                                                       \n",
       "5517094                            0.0                           0.0   \n",
       "6133770                            0.0                           0.0   \n",
       "5741413                            0.0                           0.0   \n",
       "6082466                            0.0                           1.0   \n",
       "6086244                            0.0                           1.0   \n",
       "\n",
       "                  Carrier Type_4A. SELF PRIVATE  \\\n",
       "Claim Identifier                                  \n",
       "5517094                                     0.0   \n",
       "6133770                                     0.0   \n",
       "5741413                                     0.0   \n",
       "6082466                                     0.0   \n",
       "6086244                                     0.0   \n",
       "\n",
       "                  Carrier Type_5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)  \\\n",
       "Claim Identifier                                                             \n",
       "5517094                                                                0.0   \n",
       "6133770                                                                0.0   \n",
       "5741413                                                                0.0   \n",
       "6082466                                                                0.0   \n",
       "6086244                                                                0.0   \n",
       "\n",
       "                  Carrier Type_5C. SPECIAL FUND - POI CARRIER WCB MENANDS  \\\n",
       "Claim Identifier                                                            \n",
       "5517094                                                               0.0   \n",
       "6133770                                                               0.0   \n",
       "5741413                                                               0.0   \n",
       "6082466                                                               0.0   \n",
       "6086244                                                               0.0   \n",
       "\n",
       "                  Carrier Type_5D. SPECIAL FUND - UNKNOWN  \\\n",
       "Claim Identifier                                            \n",
       "5517094                                               0.0   \n",
       "6133770                                               0.0   \n",
       "5741413                                               0.0   \n",
       "6082466                                               0.0   \n",
       "6086244                                               0.0   \n",
       "\n",
       "                  County of Injury_ALLEGANY  ...  Gender  Industry Code  \\\n",
       "Claim Identifier                             ...                          \n",
       "5517094                                 0.0  ...       1       0.217288   \n",
       "6133770                                 0.0  ...       1       0.017463   \n",
       "5741413                                 0.0  ...       1       0.015888   \n",
       "6082466                                 0.0  ...       0       0.077592   \n",
       "6086244                                 0.0  ...       0       0.077592   \n",
       "\n",
       "                  WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "Claim Identifier                                                          \n",
       "5517094                            0.050608                    0.039703   \n",
       "6133770                            0.021574                    0.021720   \n",
       "5741413                            0.108591                    0.294255   \n",
       "6082466                            0.064963                    0.193280   \n",
       "6086244                            0.020798                    0.294255   \n",
       "\n",
       "                  WCIO Part Of Body Code  Age at Injury  Average Weekly Wage  \\\n",
       "Claim Identifier                                                               \n",
       "5517094                         0.032572      -0.477828            -0.535991   \n",
       "6133770                         0.055136      -0.331088            -0.535991   \n",
       "5741413                         0.089553       1.062940            -0.535991   \n",
       "6082466                         0.055136      -0.404458            -0.535991   \n",
       "6086244                         0.083570      -1.578376            -0.535991   \n",
       "\n",
       "                  IME-4 Count  Days Between Accident_Assembly  \\\n",
       "Claim Identifier                                                \n",
       "5517094             -0.420035                       -0.131540   \n",
       "6133770             -0.420035                       -0.134855   \n",
       "5741413             -0.420035                       -0.129883   \n",
       "6082466             -0.420035                        0.027567   \n",
       "6086244             -0.420035                       -0.131540   \n",
       "\n",
       "                  Industry_Avg_Weekly_Wage  \n",
       "Claim Identifier                            \n",
       "5517094                          -0.253801  \n",
       "6133770                          -0.023187  \n",
       "5741413                           1.980481  \n",
       "6082466                          -0.856314  \n",
       "6086244                          -0.856314  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Identifier\n",
       "5517094    2. NON-COMP\n",
       "6133770    2. NON-COMP\n",
       "5741413    2. NON-COMP\n",
       "6082466    2. NON-COMP\n",
       "6086244    3. MED ONLY\n",
       "Name: Claim Injury Type, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encode Target Variable\n",
    "Label Encoder for target variable (training and validation):\n",
    "<br/> <br/>\n",
    "(This needs to be done in both the proprocessing notebook as well as here to be able to interpret the results properly when a model is tested.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Fit the encoder on the training target variable\n",
    "Y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "#Transform the training and validation target variable\n",
    "Y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_val_unencoded = y_train.copy()\n",
    "\n",
    "#Convert the results back to DataFrames while overriding the previous variable names\n",
    "y_train = pd.DataFrame(Y_train_encoded, columns=['encoded_target'], index=pd.Series(y_train.index))\n",
    "y_val = pd.DataFrame(Y_val_encoded, columns=['encoded_target'], index=pd.Series(y_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the encoded variables back to the x set\n",
    "# training_data_undersampled = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_classes = {}\n",
    "# for x in range(0,8):\n",
    "#     if x != 6:\n",
    "#         majority_classes[x] = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == x]\n",
    "\n",
    "# minority_class = training_data_undersampled[training_data_undersampled[\"encoded_target\"] == 6]\n",
    "\n",
    "# size = int(len(minority_class) + (len(minority_class) * 2))\n",
    "\n",
    "# print(size)\n",
    "\n",
    "# # Perform undersampling\n",
    "# undersampled_majority_0 = majority_classes[0].sample(n=size, random_state=42)\n",
    "# undersampled_majority_1 = majority_classes[1].sample(n=size, random_state=42)\n",
    "# undersampled_majority_2 = majority_classes[2].sample(n=size, random_state=42)\n",
    "# undersampled_majority_3 = majority_classes[3].sample(n=size, random_state=42)\n",
    "# undersampled_majority_4 = majority_classes[4].sample(n=size, random_state=42)\n",
    "# undersampled_majority_5 = majority_classes[5].sample(n=size, random_state=42)\n",
    "# undersampled_majority_7 = majority_classes[7].sample(n=size, random_state=42)\n",
    "# # undersampled_majority.head()\n",
    "# balanced_data = pd.concat([undersampled_majority_0, undersampled_majority_1, undersampled_majority_2, \n",
    "#                            undersampled_majority_3, undersampled_majority_4, undersampled_majority_5, \n",
    "#                            minority_class, undersampled_majority_7])\n",
    "\n",
    "# # Separate features and target\n",
    "# X_train = balanced_data.drop(columns='encoded_target')\n",
    "# y_train = balanced_data['encoded_target']\n",
    "\n",
    "# # Check class distribution after undersampling\n",
    "# print(\"Class distribution after undersampling:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling: encoded_target\n",
      "0                 203607\n",
      "1                 203607\n",
      "2                 203607\n",
      "3                 203607\n",
      "4                 203607\n",
      "5                 203607\n",
      "6                 203607\n",
      "7                 203607\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Fit and resample the dataset\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class distribution after oversampling:\", y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "## 2. Model\n",
    "</a>\n",
    "\n",
    "Type of Problem <br/>\n",
    "The type of problem to be solved is a multiclassification problem where the output is between 8 different choices. We will use a simple Logistical Regression model set to be able to compute multiple classes.<br/>\n",
    "<br/>\n",
    "Metric used:<br/>\n",
    "As a classification problem, we observed the following metrics to determine the effectiveness of our model:\n",
    " - accuracy\n",
    " - precision\n",
    " - recall\n",
    " - f1 score\n",
    "\n",
    " Each point is measured in a different and observing them all allows us to get an accurate view of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help display metrics for all models\n",
    "\n",
    "# helper method for score_model - not to be used seperately\n",
    "def print_scores(per_class):\n",
    "    for x,y in zip(per_class, np.unique(y_val_unencoded)):\n",
    "        if str(y) == \"7. PTD\": # add an extra tab for better alignment\n",
    "            print(\"[\"+str(y)+\"]:     \\t\\t\" + str(round(x,2))) \n",
    "        else:\n",
    "            print(\"[\"+str(y)+\"]:     \\t\" + str(round(x,2)))\n",
    "\n",
    "# displays the scores for Precision, Recall, and F1\n",
    "def score_model(y_actual, y_predicted, score_train, score_test):\n",
    "\n",
    "    print(\"------------ F1 ------------\")\n",
    "    f1_per_class = f1_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(f1_per_class)#, y_actual)\n",
    "    f1_per_weighted = f1_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro f1: \" + str(round(f1_per_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"------ Individual Score Comparisons ------ \")\n",
    "    print(\"Train Score: \" + str(score_train))\n",
    "    print(\"Test Score: \" + str(score_test))\n",
    "    diff = np.abs(score_train - score_test)\n",
    "    print(\"Difference: \" + str(diff))\n",
    "\n",
    "    print(\"--------- Accuracy ---------\\n\")\n",
    "    acc_score = accuracy_score(y_actual, y_predicted)\n",
    "    print(\"Accuracy Score: \" + str(acc_score) + \"\\n\")\n",
    "\n",
    "    print(\"--------- Precision ---------\")\n",
    "    precision_per_class = precision_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(precision_per_class)#, y_actual)\n",
    "    precision_weighted = precision_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro precision: \" + str(round(precision_weighted, 3)) + \"\\n\")\n",
    "\n",
    "    print(\"---------- Recall ----------\")\n",
    "    recall_per_class = recall_score(y_actual, y_predicted, average=None)\n",
    "    print_scores(recall_per_class)#, y_actual)\n",
    "    recall_per_weighted = recall_score(y_actual, y_predicted, average='macro')\n",
    "    print(\"\\nMacro recall: \" + str(round(recall_per_weighted, 3)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.3\n",
      "[2. NON-COMP]:     \t0.83\n",
      "[3. MED ONLY]:     \t0.25\n",
      "[4. TEMPORARY]:     \t0.4\n",
      "[5. PPD SCH LOSS]:     \t0.57\n",
      "[6. PPD NSL]:     \t0.12\n",
      "[7. PTD]:     \t\t0.01\n",
      "[8. DEATH]:     \t0.11\n",
      "\n",
      "Macro f1: 0.323\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.694273158584921\n",
      "Test Score: 0.5810066895846883\n",
      "Difference: 0.11326646900023274\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.5810066895846883\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.19\n",
      "[2. NON-COMP]:     \t0.88\n",
      "[3. MED ONLY]:     \t0.23\n",
      "[4. TEMPORARY]:     \t0.74\n",
      "[5. PPD SCH LOSS]:     \t0.47\n",
      "[6. PPD NSL]:     \t0.07\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.06\n",
      "\n",
      "Macro precision: 0.33\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.72\n",
      "[2. NON-COMP]:     \t0.78\n",
      "[3. MED ONLY]:     \t0.27\n",
      "[4. TEMPORARY]:     \t0.28\n",
      "[5. PPD SCH LOSS]:     \t0.72\n",
      "[6. PPD NSL]:     \t0.63\n",
      "[7. PTD]:     \t\t0.34\n",
      "[8. DEATH]:     \t0.8\n",
      "\n",
      "Macro recall: 0.568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.379\n",
    "# (oversampling) - 0.323 - 40s\n",
    "\n",
    "# Create the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = lr_model.score(X_train, y_train)\n",
    "score_test = lr_model.score(X_val, y_val)\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "lr_y_pred = lr_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, lr_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DecisionTreeClassifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],                          # Split criterion\n",
    "#     'splitter': ['best', 'random'],                             # Splitting strategy\n",
    "#     'max_depth': [None, 10, 20, 30],                            # Max depth of the tree\n",
    "#     'min_samples_split': [2, 5, 10],                            # Minimum samples to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4],                              # Minimum samples at a leaf node\n",
    "#     'max_features': [None, 'sqrt', 'log2'],                     # Max features to consider for splits\n",
    "#     'max_leaf_nodes': [None, 10, 20, 30],                       # Max number of leaf nodes\n",
    "#     'min_impurity_decrease': [0.0, 0.1, 0.2]                   # Minimum impurity decrease to split\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV with 5-fold cross-validation and scoring based on accuracy\n",
    "# grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# # You can also access the best model found\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# #Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# #Best Score: 0.7769977245887005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.31\n",
      "[2. NON-COMP]:     \t0.83\n",
      "[3. MED ONLY]:     \t0.19\n",
      "[4. TEMPORARY]:     \t0.56\n",
      "[5. PPD SCH LOSS]:     \t0.59\n",
      "[6. PPD NSL]:     \t0.11\n",
      "[7. PTD]:     \t\t0.01\n",
      "[8. DEATH]:     \t0.14\n",
      "\n",
      "Macro f1: 0.343\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.7502615332478746\n",
      "Test Score: 0.6299939607916009\n",
      "Difference: 0.12026757245627373\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.6299939607916009\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.2\n",
      "[2. NON-COMP]:     \t0.86\n",
      "[3. MED ONLY]:     \t0.27\n",
      "[4. TEMPORARY]:     \t0.72\n",
      "[5. PPD SCH LOSS]:     \t0.48\n",
      "[6. PPD NSL]:     \t0.06\n",
      "[7. PTD]:     \t\t0.01\n",
      "[8. DEATH]:     \t0.08\n",
      "\n",
      "Macro precision: 0.335\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.74\n",
      "[2. NON-COMP]:     \t0.81\n",
      "[3. MED ONLY]:     \t0.14\n",
      "[4. TEMPORARY]:     \t0.46\n",
      "[5. PPD SCH LOSS]:     \t0.75\n",
      "[6. PPD NSL]:     \t0.57\n",
      "[7. PTD]:     \t\t0.34\n",
      "[8. DEATH]:     \t0.72\n",
      "\n",
      "Macro recall: 0.566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.366\n",
    "# (oversampling) - 0.343 - 21s\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  # 'gini' for Gini Impurity or 'entropy' for Information Gain\n",
    "    max_depth=10, \n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease= 0.0,\n",
    "    min_samples_leaf= 1,\n",
    "    min_samples_split=2,\n",
    "    splitter='best',  # Maximum depth of the tree (None means no limit)\n",
    "    random_state=42    # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = decision_tree.score(X_train, y_train)\n",
    "score_test = decision_tree.score(X_val, y_val)\n",
    "\n",
    "# Make predictions\n",
    "dt_y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, dt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for KNN\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 10, 15],                 \n",
    "#     'algorithm': ['brute', 'kd_tree'],             \n",
    "#     'metric': ['euclidean', 'manhattan', 'minkowski'], \n",
    "#     'weights': ['uniform', 'distance']             \n",
    "# }\n",
    "\n",
    "# # Set up the GridSearchCV with KNN classifier\n",
    "# grid_search = GridSearchCV(\n",
    "#     KNeighborsClassifier(),\n",
    "#     param_grid,\n",
    "#     cv=5,                                         \n",
    "#     scoring='f1_macro'                                                      \n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - KNN<br/>\n",
    "KNN takes too long to process due to our large dataset so will be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.334\n",
    "\n",
    "# # Create the KNN model\n",
    "# # n_neighbors specifies the number of neighbors to use for classification\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5)  \n",
    "\n",
    "# # Fit the model to the training set\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Determine the scores for the model for both train and validation sets\n",
    "# score_train = knn_model.score(X_train, y_train)\n",
    "# score_test = knn_model.score(X_val, y_val)\n",
    "\n",
    "# # Use the model to predict on the validation set\n",
    "# knn_y_pred = knn_model.predict(X_val)\n",
    "\n",
    "# # Display the model metrics using the score_model function\n",
    "# score_model(y_val, knn_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEURAL NETWORK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch - MLPClasssifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(64,), (64, 32), (128, 64)],  # Different hidden layer architectures\n",
    "#     'activation': ['relu', 'tanh'],                     # Activation functions\n",
    "#     'solver': ['adam', 'sgd'],                          # Optimizers\n",
    "#     'alpha': [0.0001, 0.001, 0.01],                     # L2 regularization (alpha)\n",
    "#     'learning_rate_init': [0.001, 0.01, 0.1],           # Learning rates\n",
    "# }\n",
    "\n",
    "# # Create the MLPClassifier model\n",
    "# mlp = MLPClassifier(max_iter=200, random_state=42)  # Keeping max_iter constant at 200\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=mlp,\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=5,  # 5-fold cross-validation\n",
    "#                            scoring='f1_macro',  # Evaluation metric\n",
    "#                            verbose=2,           # Display progress logs\n",
    "#                            n_jobs=-1)           # Use all available processors\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.48\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.24\n",
      "[4. TEMPORARY]:     \t0.68\n",
      "[5. PPD SCH LOSS]:     \t0.62\n",
      "[6. PPD NSL]:     \t0.12\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.25\n",
      "\n",
      "Macro f1: 0.407\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.8212678100458236\n",
      "Test Score: 0.7070112886741615\n",
      "Difference: 0.11425652137166209\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7070112886741615\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.37\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.3\n",
      "[4. TEMPORARY]:     \t0.73\n",
      "[5. PPD SCH LOSS]:     \t0.53\n",
      "[6. PPD NSL]:     \t0.08\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.2\n",
      "\n",
      "Macro precision: 0.384\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.66\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.2\n",
      "[4. TEMPORARY]:     \t0.63\n",
      "[5. PPD SCH LOSS]:     \t0.74\n",
      "[6. PPD NSL]:     \t0.3\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.35\n",
      "\n",
      "Macro recall: 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#f1 score of 0.415\n",
    "# (oversampling) - 0.407 - (0.11 diff in scores) - 12m 3s \n",
    "\n",
    "# Create the model\n",
    "mlpc_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility\n",
    "\n",
    "# Fit the model to the training set\n",
    "mlpc_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = mlpc_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = mlpc_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "mplc_y_pred = mlpc_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, mplc_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Random Forest</a> -> (overfits) <br/>\n",
    "Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.59\n",
      "[2. NON-COMP]:     \t0.89\n",
      "[3. MED ONLY]:     \t0.2\n",
      "[4. TEMPORARY]:     \t0.75\n",
      "[5. PPD SCH LOSS]:     \t0.63\n",
      "[6. PPD NSL]:     \t0.03\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.36\n",
      "\n",
      "Macro f1: 0.433\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.9999889492993855\n",
      "Test Score: 0.7632107683731302\n",
      "Difference: 0.23677818092625535\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7632107683731302\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.63\n",
      "[2. NON-COMP]:     \t0.86\n",
      "[3. MED ONLY]:     \t0.36\n",
      "[4. TEMPORARY]:     \t0.71\n",
      "[5. PPD SCH LOSS]:     \t0.6\n",
      "[6. PPD NSL]:     \t0.16\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.53\n",
      "\n",
      "Macro precision: 0.481\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.55\n",
      "[2. NON-COMP]:     \t0.92\n",
      "[3. MED ONLY]:     \t0.14\n",
      "[4. TEMPORARY]:     \t0.81\n",
      "[5. PPD SCH LOSS]:     \t0.67\n",
      "[6. PPD NSL]:     \t0.02\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.28\n",
      "\n",
      "Macro recall: 0.423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.379\n",
    "# (oversampling) - 0.433 (overfitting w/ 0.23 diff) - 6m 5s\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = rf_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = rf_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "rf_y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, rf_y_pred, score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight = np.sum(y_train == 6) / np.sum(y_train != 6)\n",
    "\n",
    "# score = 0\n",
    "# score_settings = \"\"\n",
    "\n",
    "# for x in range(1,20):\n",
    "#     for y in range(50, 151, 10):\n",
    "#         for z in np.arange(0, 1.1, 0.1):\n",
    "#             xgb_model = xgb.XGBClassifier(\n",
    "#                 n_estimators=y,  # Number of trees\n",
    "#                 learning_rate=z,  # Step size shrinkage\n",
    "#                 max_depth=x,       # Maximum depth of a tree\n",
    "#                 random_state=42,   # For reproducibility\n",
    "#                 use_label_encoder=False,  # Avoid warning for encoding\n",
    "#                 eval_metric='mlogloss',    # Evaluation metric for multi-class classification\n",
    "#                 scale_pos_weight = pos_weight\n",
    "#             )\n",
    "#             xgb_model.fit(X_train, y_train)\n",
    "#             xgb_y_pred = xgb_model.predict(X_val)\n",
    "#             f1 = f1_score(y_val, xgb_y_pred, average=\"macro\")\n",
    "\n",
    "#             if f1 > score:\n",
    "#                 score = f1\n",
    "#                 score_settings = \"max_depth: \" + str(x) + \" | n_estimators: \" + str(y) + \" | lr: \" + str(z)\n",
    "\n",
    "# print(score)\n",
    "# print(score_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://xgboost.readthedocs.io/en/stable/tutorials/index.html\">XGBoost</a> -> (tends to overfit): <br/>\n",
    "Also using decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.58\n",
      "[2. NON-COMP]:     \t0.9\n",
      "[3. MED ONLY]:     \t0.22\n",
      "[4. TEMPORARY]:     \t0.76\n",
      "[5. PPD SCH LOSS]:     \t0.64\n",
      "[6. PPD NSL]:     \t0.11\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.43\n",
      "\n",
      "Macro f1: 0.453\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.8689552667639129\n",
      "Test Score: 0.7650631794109449\n",
      "Difference: 0.10389208735296795\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7650631794109449\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.57\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.39\n",
      "[4. TEMPORARY]:     \t0.72\n",
      "[5. PPD SCH LOSS]:     \t0.59\n",
      "[6. PPD NSL]:     \t0.13\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.34\n",
      "\n",
      "Macro precision: 0.451\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.59\n",
      "[2. NON-COMP]:     \t0.93\n",
      "[3. MED ONLY]:     \t0.15\n",
      "[4. TEMPORARY]:     \t0.79\n",
      "[5. PPD SCH LOSS]:     \t0.7\n",
      "[6. PPD NSL]:     \t0.09\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.57\n",
      "\n",
      "Macro recall: 0.478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.442\n",
    "# (oversampling) 0.453 (overfit by 0.10 diff) 4m 18s\n",
    "\n",
    "# max_depth = 19, n_estimators = 150, lr = 0.6 -> overfitting\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = xgb_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = xgb_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "xgb_y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, xgb_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 min = max_depth = 6 - .402 f1\n",
    "gbdt_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Number of boosting stages\n",
    "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
    "    max_depth=6,            # Limits depth of each tree to prevent overfitting\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine the scores for the model for both train and validation sets\n",
    "score_train = gbdt_model.score(X_train, y_train)  # Accuracy on training data\n",
    "score_test = gbdt_model.score(X_val, y_val)      # Accuracy on validation data\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "gbdt_y_pred = gbdt_model.predict(X_val)\n",
    "\n",
    "# Display the model metrics using the score_model function\n",
    "score_model(y_val, gbdt_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.46\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.2\n",
      "[4. TEMPORARY]:     \t0.71\n",
      "[5. PPD SCH LOSS]:     \t0.58\n",
      "[6. PPD NSL]:     \t0.09\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.33\n",
      "\n",
      "Macro f1: 0.405\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.995443427779988\n",
      "Test Score: 0.701634070426461\n",
      "Difference: 0.293809357353527\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.701634070426461\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.38\n",
      "[2. NON-COMP]:     \t0.86\n",
      "[3. MED ONLY]:     \t0.22\n",
      "[4. TEMPORARY]:     \t0.71\n",
      "[5. PPD SCH LOSS]:     \t0.56\n",
      "[6. PPD NSL]:     \t0.1\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.31\n",
      "\n",
      "Macro precision: 0.391\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.6\n",
      "[2. NON-COMP]:     \t0.85\n",
      "[3. MED ONLY]:     \t0.19\n",
      "[4. TEMPORARY]:     \t0.71\n",
      "[5. PPD SCH LOSS]:     \t0.62\n",
      "[6. PPD NSL]:     \t0.09\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.35\n",
      "\n",
      "Macro recall: 0.425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oversampling - 0.405 - 6m 55s - overfitting by 0.29\n",
    "# 0.4 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ F1 ------------\n",
      "[1. CANCELLED]:     \t0.58\n",
      "[2. NON-COMP]:     \t0.89\n",
      "[3. MED ONLY]:     \t0.22\n",
      "[4. TEMPORARY]:     \t0.76\n",
      "[5. PPD SCH LOSS]:     \t0.64\n",
      "[6. PPD NSL]:     \t0.11\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.42\n",
      "\n",
      "Macro f1: 0.451\n",
      "\n",
      "------ Individual Score Comparisons ------ \n",
      "Train Score: 0.869213116444916\n",
      "Test Score: 0.7642792437052867\n",
      "Difference: 0.10493387273962929\n",
      "--------- Accuracy ---------\n",
      "\n",
      "Accuracy Score: 0.7642792437052867\n",
      "\n",
      "--------- Precision ---------\n",
      "[1. CANCELLED]:     \t0.56\n",
      "[2. NON-COMP]:     \t0.87\n",
      "[3. MED ONLY]:     \t0.39\n",
      "[4. TEMPORARY]:     \t0.72\n",
      "[5. PPD SCH LOSS]:     \t0.6\n",
      "[6. PPD NSL]:     \t0.13\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.32\n",
      "\n",
      "Macro precision: 0.448\n",
      "\n",
      "---------- Recall ----------\n",
      "[1. CANCELLED]:     \t0.59\n",
      "[2. NON-COMP]:     \t0.92\n",
      "[3. MED ONLY]:     \t0.15\n",
      "[4. TEMPORARY]:     \t0.8\n",
      "[5. PPD SCH LOSS]:     \t0.7\n",
      "[6. PPD NSL]:     \t0.09\n",
      "[7. PTD]:     \t\t0.0\n",
      "[8. DEATH]:     \t0.59\n",
      "\n",
      "Macro recall: 0.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oversampling with correct hyperparameters - 0.451 - 0.104 for overfitting\n",
    "# oversampling - 0.448 - 17m 59s - 0.107 diff for overfitting\n",
    "# 0.425 f1 macro score\n",
    "bagging_model = BaggingClassifier(estimator=xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    "), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hidden_layer_sizes=(13,), max_iter=500, random_state=42) - 0.395 (no overfit) 12m 58s\n",
    "# (hidden_layer_sizes=(15,), max_iter=500, random_state=42) - 0.407 (no overfit) 11m 34s\n",
    "# (hidden_layer_sizes=(20,), max_iter=500, random_state=42) - 0.407 (no overfit) 15m 20s\n",
    "# (hidden_layer_sizes=(10,), max_iter=500, random_state=42) - 0.389 (no overfit) 4m 6s\n",
    "# (hidden_layer_sizes=(10,), max_iter=1000, random_state=42) - 0.389 (no overfit) 4m 8s\n",
    "base_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42)             # For reproducibility)\n",
    "bagging_model = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "score_train = bagging_model.score(X_train, y_train)\n",
    "score_test = bagging_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, bagging_y_pred, score_train, score_test)\n",
    "# try this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.440 - LR -> XGB -> MLP w/ a 0.015 difference in scores (3m 55s)\n",
    "# 0.425 - LR -> MLP -> XGB w/ a 0.0099 difference in scores (37m)\n",
    "# 0.410 - MLP -> XGB -> GBC w/ a 0.011 difference in scores (93m 35s)\n",
    "\n",
    "base_models = [\n",
    "    ('mlpc', LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "    n_estimators=110,  # Number of trees\n",
    "    learning_rate=0.2,  # Step size shrinkage\n",
    "    max_depth=7,       # Maximum depth of a tree\n",
    "    random_state=42,   # For reproducibility\n",
    "    use_label_encoder=False,  # Avoid warning for encoding\n",
    "    eval_metric='mlogloss'    # Evaluation metric for multi-class classification\n",
    ") )\n",
    "]\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "                      activation='relu',           # ReLU activation function\n",
    "                      solver='adam',               # Adam optimizer\n",
    "                      alpha=0.0001,                # Regularization term (L2 penalty)\n",
    "                      learning_rate_init=0.001,    # Initial learning rate\n",
    "                      max_iter=200,                # Maximum number of iterations\n",
    "                      random_state=42) \n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=nn)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred = stacked_model.predict(X_val)\n",
    "\n",
    "score_train = stacked_model.score(X_train, y_train)\n",
    "score_test = stacked_model.score(X_val, y_val)\n",
    "\n",
    "score_model(y_val, y_pred, score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lr_y_pred_f1   = f1_score(y_val, lr_y_pred, average='macro')\n",
    "# # dt_y_pred_f1   = f1_score(y_val, dt_y_pred, average='macro')\n",
    "# # knn_y_pred_f1  = f1_score(y_val, knn_y_pred, average='macro')\n",
    "# mplc_y_pred_f1 = f1_score(y_val, mplc_y_pred, average='macro')\n",
    "# # rf_y_pred_f1   = f1_score(y_val, rf_y_pred, average='macro')\n",
    "# xgb_y_pred_f1  = f1_score(y_val, xgb_y_pred, average='macro')\n",
    "# gbdt_y_pred_f1 = f1_score(y_val, gbdt_y_pred, average='macro')\n",
    "\n",
    "# # f1_score(y_actual, y_predicted, average='macro')\n",
    "\n",
    "# # Assign weights based on F1 scores\n",
    "# #weights = [lr_y_pred_f1, dt_y_pred_f1, knn_y_pred_f1, mplc_y_pred_f1, rf_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = [mplc_y_pred_f1, xgb_y_pred_f1, gbdt_y_pred_f1]\n",
    "# weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "\n",
    "# # Make weighted predictions\n",
    "# # lr_probs    = lr_model.predict_proba(X_val)[:, 1]\n",
    "# # dt_probs    = decision_tree.predict_proba(X_val)[:, 1]\n",
    "# # knn_probs   = knn_model.predict_proba(X_val)[:, 1]\n",
    "# mplc_probs  = mlpc_model.predict_proba(X_val)[:, 1]\n",
    "# # rf_probs    = rf_model.predict_proba(X_val)[:, 1]\n",
    "# xgb_probs   = xgb_model.predict_proba(X_val)[:, 1]\n",
    "# gbdt_probs  = gbdt_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# # Aggregate predictions using weights\n",
    "# weighted_probs = (\n",
    "#                     # weights[0] * lr_probs +\n",
    "#                 #   weights[1] * dt_probs +\n",
    "#                 #   weights[2] * knn_probs +\n",
    "#                   weights[0] * mplc_probs + \n",
    "#                 #   weights[4] * rf_probs + \n",
    "#                   weights[1] * xgb_probs + \n",
    "#                   weights[2] * gbdt_probs)\n",
    "\n",
    "# # Final predictions (threshold = 0.5)\n",
    "# final_predictions = (weighted_probs >= 0.2).astype(int)\n",
    "\n",
    "# # Evaluate the ensemble\n",
    "# final_f1 = f1_score(y_val, final_predictions, average='macro')\n",
    "# print(f\"Weighted Ensemble F1 Score: {final_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kaggle\">\n",
    "\n",
    "## 11. Kaggle Submission\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "# y_pred_test = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode the prediction labels back to their original values\n",
    "# decoded_labels = label_encoder.inverse_transform(y_pred_test)\n",
    "# decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the prediction values with their claim identifiers into a dataframe\n",
    "# kaggle_submission = pd.DataFrame({\"Claim Identifier\": test_data.index, \"Claim Injury Type\":decoded_labels})\n",
    "# kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the resulting dataframe into a csv file named \"Kaggle_submission.csv\"\n",
    "# this will be found in the directory the file is currently running from\n",
    "# if a file exists with the same name, it will overwrite it with the new output.\n",
    "# kaggle_submission.to_csv(\"Kaggle_Submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
